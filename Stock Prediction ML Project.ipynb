

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import plotly.express as px
from keras.preprocessing.sequence import TimeseriesGenerator
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score
import warnings
warnings.filterwarnings('ignore')
from keras.models import Sequential
from keras.layers import Dense,LSTM,Dropout
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.layers import Dense,LSTM,Dropout,GRU

%matplotlib inline
     

Data = pd.read_csv('/content/GOOG.csv',parse_dates =True)

     

df = pd.DataFrame(Data)
df1 =df.copy()
df
df.describe().T
df.info()

     
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 1258 entries, 0 to 1257
Data columns (total 14 columns):
 #   Column       Non-Null Count  Dtype  
---  ------       --------------  -----  
 0   symbol       1258 non-null   object 
 1   date         1258 non-null   object 
 2   close        1258 non-null   float64
 3   high         1258 non-null   float64
 4   low          1258 non-null   float64
 5   open         1258 non-null   float64
 6   volume       1258 non-null   int64  
 7   adjClose     1258 non-null   float64
 8   adjHigh      1258 non-null   float64
 9   adjLow       1258 non-null   float64
 10  adjOpen      1258 non-null   float64
 11  adjVolume    1258 non-null   int64  
 12  divCash      1258 non-null   float64
 13  splitFactor  1258 non-null   float64
dtypes: float64(10), int64(2), object(2)
memory usage: 137.7+ KB

df
     
symbol	date	close	high	low	open	volume	adjClose	adjHigh	adjLow	adjOpen	adjVolume	divCash	splitFactor
0	GOOG	2016-06-14 00:00:00+00:00	718.27	722.470	713.1200	716.48	1306065	718.27	722.470	713.1200	716.48	1306065	0.0	1.0
1	GOOG	2016-06-15 00:00:00+00:00	718.92	722.980	717.3100	719.00	1214517	718.92	722.980	717.3100	719.00	1214517	0.0	1.0
2	GOOG	2016-06-16 00:00:00+00:00	710.36	716.650	703.2600	714.91	1982471	710.36	716.650	703.2600	714.91	1982471	0.0	1.0
3	GOOG	2016-06-17 00:00:00+00:00	691.72	708.820	688.4515	708.65	3402357	691.72	708.820	688.4515	708.65	3402357	0.0	1.0
4	GOOG	2016-06-20 00:00:00+00:00	693.71	702.480	693.4100	698.77	2082538	693.71	702.480	693.4100	698.77	2082538	0.0	1.0
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
1253	GOOG	2021-06-07 00:00:00+00:00	2466.09	2468.000	2441.0725	2451.32	1192453	2466.09	2468.000	2441.0725	2451.32	1192453	0.0	1.0
1254	GOOG	2021-06-08 00:00:00+00:00	2482.85	2494.495	2468.2400	2479.90	1253253	2482.85	2494.495	2468.2400	2479.90	1253253	0.0	1.0
1255	GOOG	2021-06-09 00:00:00+00:00	2491.40	2505.000	2487.3300	2499.50	1006337	2491.40	2505.000	2487.3300	2499.50	1006337	0.0	1.0
1256	GOOG	2021-06-10 00:00:00+00:00	2521.60	2523.260	2494.0000	2494.01	1561733	2521.60	2523.260	2494.0000	2494.01	1561733	0.0	1.0
1257	GOOG	2021-06-11 00:00:00+00:00	2513.93	2526.990	2498.2900	2524.92	1262309	2513.93	2526.990	2498.2900	2524.92	1262309	0.0	1.0
1258 rows × 14 columns


df['Date'] =df['date'].str.split(' ').str.get(0)
df
     
symbol	date	close	high	low	open	volume	adjClose	adjHigh	adjLow	adjOpen	adjVolume	divCash	splitFactor	Date
0	GOOG	2016-06-14 00:00:00+00:00	718.27	722.470	713.1200	716.48	1306065	718.27	722.470	713.1200	716.48	1306065	0.0	1.0	2016-06-14
1	GOOG	2016-06-15 00:00:00+00:00	718.92	722.980	717.3100	719.00	1214517	718.92	722.980	717.3100	719.00	1214517	0.0	1.0	2016-06-15
2	GOOG	2016-06-16 00:00:00+00:00	710.36	716.650	703.2600	714.91	1982471	710.36	716.650	703.2600	714.91	1982471	0.0	1.0	2016-06-16
3	GOOG	2016-06-17 00:00:00+00:00	691.72	708.820	688.4515	708.65	3402357	691.72	708.820	688.4515	708.65	3402357	0.0	1.0	2016-06-17
4	GOOG	2016-06-20 00:00:00+00:00	693.71	702.480	693.4100	698.77	2082538	693.71	702.480	693.4100	698.77	2082538	0.0	1.0	2016-06-20
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
1253	GOOG	2021-06-07 00:00:00+00:00	2466.09	2468.000	2441.0725	2451.32	1192453	2466.09	2468.000	2441.0725	2451.32	1192453	0.0	1.0	2021-06-07
1254	GOOG	2021-06-08 00:00:00+00:00	2482.85	2494.495	2468.2400	2479.90	1253253	2482.85	2494.495	2468.2400	2479.90	1253253	0.0	1.0	2021-06-08
1255	GOOG	2021-06-09 00:00:00+00:00	2491.40	2505.000	2487.3300	2499.50	1006337	2491.40	2505.000	2487.3300	2499.50	1006337	0.0	1.0	2021-06-09
1256	GOOG	2021-06-10 00:00:00+00:00	2521.60	2523.260	2494.0000	2494.01	1561733	2521.60	2523.260	2494.0000	2494.01	1561733	0.0	1.0	2021-06-10
1257	GOOG	2021-06-11 00:00:00+00:00	2513.93	2526.990	2498.2900	2524.92	1262309	2513.93	2526.990	2498.2900	2524.92	1262309	0.0	1.0	2021-06-11
1258 rows × 15 columns


df.drop(columns=['date','symbol'],inplace=True)
df['Date']= pd.to_datetime(df['Date'])
df = df.set_index('Date')
df
     
close	high	low	open	volume	adjClose	adjHigh	adjLow	adjOpen	adjVolume	divCash	splitFactor
Date												
2016-06-14	718.27	722.470	713.1200	716.48	1306065	718.27	722.470	713.1200	716.48	1306065	0.0	1.0
2016-06-15	718.92	722.980	717.3100	719.00	1214517	718.92	722.980	717.3100	719.00	1214517	0.0	1.0
2016-06-16	710.36	716.650	703.2600	714.91	1982471	710.36	716.650	703.2600	714.91	1982471	0.0	1.0
2016-06-17	691.72	708.820	688.4515	708.65	3402357	691.72	708.820	688.4515	708.65	3402357	0.0	1.0
2016-06-20	693.71	702.480	693.4100	698.77	2082538	693.71	702.480	693.4100	698.77	2082538	0.0	1.0
...	...	...	...	...	...	...	...	...	...	...	...	...
2021-06-07	2466.09	2468.000	2441.0725	2451.32	1192453	2466.09	2468.000	2441.0725	2451.32	1192453	0.0	1.0
2021-06-08	2482.85	2494.495	2468.2400	2479.90	1253253	2482.85	2494.495	2468.2400	2479.90	1253253	0.0	1.0
2021-06-09	2491.40	2505.000	2487.3300	2499.50	1006337	2491.40	2505.000	2487.3300	2499.50	1006337	0.0	1.0
2021-06-10	2521.60	2523.260	2494.0000	2494.01	1561733	2521.60	2523.260	2494.0000	2494.01	1561733	0.0	1.0
2021-06-11	2513.93	2526.990	2498.2900	2524.92	1262309	2513.93	2526.990	2498.2900	2524.92	1262309	0.0	1.0
1258 rows × 12 columns


font1 = {'family':'serif','size':18}
font2 = {'family':'serif','size':15}
font3 = {'family':'serif','size':13}
     

colors =['blue','Red', 'Yellow','turquoise','blue','Red', 'Yellow','turquoise', 'blue','Red', 'Yellow','turquoise']
colors= ['lightskyblue' , 'lightpink' , 'cadetblue','lightskyblue' , 'lightpink' , 'cadetblue','lightskyblue' , 'lightpink' , 'cadetblue','lightskyblue' , 'lightpink' , 'cadetblue']
f = plt.figure()
f.set_figwidth(20)
f.set_figheight(40)

plt.subplots_adjust(left=0.1,
                    bottom=0.1,
                    right=0.9,
                    top=0.9,
                    wspace=0.2,
                    hspace=0.5)
i=1
for column in df.columns:
    plt.subplot(6,2,i)
    plt.plot(df[column], color=colors[i-1])
    plt.title(column,backgroundcolor='grey',color='white',fontdict=font1)
    plt.xticks(fontsize=13)
    plt.yticks(fontsize=13)
    plt.xlabel('Year',fontdict=font2,labelpad=15)
    plt.ylabel("Price",fontdict=font2,labelpad=15)
    plt.grid()
    i+=1
     


df2 = df.drop(columns=['volume','divCash','splitFactor','adjVolume'])
df2
     
close	high	low	open	adjClose	adjHigh	adjLow	adjOpen
Date								
2016-06-14	718.27	722.470	713.1200	716.48	718.27	722.470	713.1200	716.48
2016-06-15	718.92	722.980	717.3100	719.00	718.92	722.980	717.3100	719.00
2016-06-16	710.36	716.650	703.2600	714.91	710.36	716.650	703.2600	714.91
2016-06-17	691.72	708.820	688.4515	708.65	691.72	708.820	688.4515	708.65
2016-06-20	693.71	702.480	693.4100	698.77	693.71	702.480	693.4100	698.77
...	...	...	...	...	...	...	...	...
2021-06-07	2466.09	2468.000	2441.0725	2451.32	2466.09	2468.000	2441.0725	2451.32
2021-06-08	2482.85	2494.495	2468.2400	2479.90	2482.85	2494.495	2468.2400	2479.90
2021-06-09	2491.40	2505.000	2487.3300	2499.50	2491.40	2505.000	2487.3300	2499.50
2021-06-10	2521.60	2523.260	2494.0000	2494.01	2521.60	2523.260	2494.0000	2494.01
2021-06-11	2513.93	2526.990	2498.2900	2524.92	2513.93	2526.990	2498.2900	2524.92
1258 rows × 8 columns


fig,ax = plt.subplots(figsize=(15,10))
df2.plot(ax = ax,alpha=0.5)
ax.set_title('Stock Price',backgroundcolor='grey',color='white',fontdict=font1)
ax.set_xlabel('Year',fontdict=font2,labelpad=15)
ax.set_ylabel("Price",fontdict=font2,labelpad=15)
ax.grid()
     


df1['Date'] =df1['date'].str.split(' ').str.get(0)
df1.drop(columns=['symbol','date','divCash','splitFactor'],inplace=True)
df_2016 = df1[(df1['Date']>='2016-01-01') &(df1['Date']<='2016-12-31')]
df_2016['Date']= pd.to_datetime(df_2016['Date'])
df_2016 = df_2016.set_index('Date')

     

df_2021 = df1[(df1['Date']>='2021-01-01') & (df1['Date'] <='2021-12-31')]
df_2021['Date'] = pd.to_datetime(df_2021['Date'])
df_2021 = df_2021.set_index('Date')
     

d2016 = df_2016.resample(rule ='MS').mean()
d2021 = df_2021.resample(rule ='MS').mean()

     

fig,(ax1,ax2) = plt.subplots(2,1,figsize=(15,10))
d2016[['close','high','low','open']].plot(ax=ax1)
ax1.set_title('Year 2016 Analysis ',backgroundcolor='grey',color='white',fontdict=font2)
d2021[['close','high','low','open']].plot(ax=ax2)
ax2.set_title('Year 2021 Analysis ',backgroundcolor='grey',color='white',fontdict=font2)

ax2.set_xlabel('Month',fontdict=font3)
ax1.set_xlabel('Month',fontdict=font3)
     
Text(0.5, 0, 'Month')


f = plt.figure()
f.set_figwidth(15)
f.set_figheight(40)
plt.subplots_adjust(left=0.1,
                    bottom=0.1,
                    right=0.9,
                    top=0.9,
                    wspace=0.2,
                    hspace=0.5)
i = 1
for column in df.drop(columns=['volume','divCash','splitFactor','adjVolume']).columns:
    plt.subplot(6,2,i)
    ax = df[column].resample('A').mean().plot.bar(color =['lightskyblue' , 'lightpink' , 'cadetblue','lightskyblue' , 'lightpink' , 'cadetblue'])
    plt.xticks(rotation = 45,fontsize = 13)
    plt.yticks(fontsize = 13)
    plt.title(f'Yearly end Mean {column} Price',backgroundcolor='grey',color='white',fontdict=font1)
    plt.xlabel('Date',fontdict=font2,labelpad=15)
    ax.yaxis.grid(True)
    i+=1
     


W6 = df.rolling(window=6).mean()
W30 = df.rolling(window=30).mean()
W60 = df.rolling(window=60).mean()
plt.figure(figsize=(12, 9))
df['close'].plot(label='Close Price').autoscale(axis='x',tight=True)
W6['close'].plot(label='Business days rolling').autoscale(axis='x',tight=True)
W30['close'].plot(label='30 Days rolling').autoscale(axis='x',tight=True)
W60['close'].plot(label='60 Days rolling').autoscale(axis='x',tight=True)
plt.legend()
plt.title('Moving Averages Analysis',backgroundcolor='grey',color='white',fontdict=font2, fontweight='bold')
plt.xlabel('Date',fontdict=font3,labelpad=15)
plt.ylabel('Price',fontdict=font3,labelpad=15)
plt.grid(True)
plt.tight_layout()
plt.show()
     


DF = df[['close','high','low','open']]
     

scaler = MinMaxScaler()
DF[DF.columns]= scaler.fit_transform(DF)
DF.shape
     
(1258, 4)

training_size = round(len(DF)*0.80)
train_data = DF.iloc[:training_size,0:4]
test_data = DF.iloc[training_size:,0:4]
print(train_data.shape,test_data.shape)
     
(1006, 4) (252, 4)

def prepare_time_series_data(Data,window_size):
  sequences =[]
  labels =[]
  i = 0
  for j   in range(window_size,len(Data)):
    sequences.append(Data.iloc[i:j])
    labels.append(Data.iloc[j])
    i+=1
  return np.array(sequences),np.array(labels)
     

X_train ,y_train = prepare_time_series_data(train_data,60)
X_test ,y_test = prepare_time_series_data(test_data,60)
X_train.shape,y_train.shape,X_test.shape,y_test.shape

     
((946, 60, 4), (946, 4), (192, 60, 4), (192, 4))

length = 60
LSTM1  = Sequential()
LSTM1.add(LSTM(100,return_sequences=True,input_shape=(length,X_train.shape[2])))
LSTM1.add(Dropout(0.2))
LSTM1.add(LSTM(100,return_sequences = False,input_shape=(length,X_train.shape[2])))
LSTM1.add(Dropout(0.2))
LSTM1.add(Dense(4))
LSTM1.compile(optimizer='adam',loss='mse',metrics=['mae'])
LSTM1.summary()


     
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 lstm (LSTM)                 (None, 60, 100)           42000     
                                                                 
 dropout (Dropout)           (None, 60, 100)           0         
                                                                 
 lstm_1 (LSTM)               (None, 100)               80400     
                                                                 
 dropout_1 (Dropout)         (None, 100)               0         
                                                                 
 dense (Dense)               (None, 4)                 404       
                                                                 
=================================================================
Total params: 122804 (479.70 KB)
Trainable params: 122804 (479.70 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________

early_stop = EarlyStopping(monitor='loss',patience=5)
     

LSTM1.fit(X_train,y_train,epochs=30,validation_data =(X_test,y_test),batch_size=32,callbacks=[early_stop])
     
Epoch 1/30
30/30 [==============================] - 9s 125ms/step - loss: 0.0071 - mae: 0.0577 - val_loss: 0.0190 - val_mae: 0.1184
Epoch 2/30
30/30 [==============================] - 3s 96ms/step - loss: 0.0013 - mae: 0.0271 - val_loss: 0.0084 - val_mae: 0.0779
Epoch 3/30
30/30 [==============================] - 4s 142ms/step - loss: 0.0011 - mae: 0.0244 - val_loss: 0.0076 - val_mae: 0.0731
Epoch 4/30
30/30 [==============================] - 3s 98ms/step - loss: 9.7086e-04 - mae: 0.0231 - val_loss: 0.0041 - val_mae: 0.0512
Epoch 5/30
30/30 [==============================] - 3s 98ms/step - loss: 0.0010 - mae: 0.0231 - val_loss: 0.0046 - val_mae: 0.0549
Epoch 6/30
30/30 [==============================] - 3s 98ms/step - loss: 8.5838e-04 - mae: 0.0215 - val_loss: 0.0030 - val_mae: 0.0431
Epoch 7/30
30/30 [==============================] - 4s 124ms/step - loss: 9.2680e-04 - mae: 0.0223 - val_loss: 0.0037 - val_mae: 0.0497
Epoch 8/30
30/30 [==============================] - 3s 101ms/step - loss: 8.3356e-04 - mae: 0.0214 - val_loss: 0.0059 - val_mae: 0.0659
Epoch 9/30
30/30 [==============================] - 3s 90ms/step - loss: 7.9852e-04 - mae: 0.0206 - val_loss: 0.0047 - val_mae: 0.0579
Epoch 10/30
30/30 [==============================] - 3s 95ms/step - loss: 7.4482e-04 - mae: 0.0201 - val_loss: 0.0026 - val_mae: 0.0405
Epoch 11/30
30/30 [==============================] - 3s 104ms/step - loss: 7.4990e-04 - mae: 0.0201 - val_loss: 0.0040 - val_mae: 0.0524
Epoch 12/30
30/30 [==============================] - 4s 126ms/step - loss: 7.0374e-04 - mae: 0.0196 - val_loss: 0.0042 - val_mae: 0.0539
Epoch 13/30
30/30 [==============================] - 3s 91ms/step - loss: 6.8551e-04 - mae: 0.0192 - val_loss: 0.0070 - val_mae: 0.0735
Epoch 14/30
30/30 [==============================] - 3s 95ms/step - loss: 6.6602e-04 - mae: 0.0192 - val_loss: 0.0036 - val_mae: 0.0498
Epoch 15/30
30/30 [==============================] - 3s 95ms/step - loss: 6.1889e-04 - mae: 0.0184 - val_loss: 0.0023 - val_mae: 0.0377
Epoch 16/30
30/30 [==============================] - 4s 137ms/step - loss: 6.4803e-04 - mae: 0.0188 - val_loss: 0.0020 - val_mae: 0.0346
Epoch 17/30
30/30 [==============================] - 3s 94ms/step - loss: 6.0678e-04 - mae: 0.0181 - val_loss: 0.0031 - val_mae: 0.0456
Epoch 18/30
30/30 [==============================] - 3s 95ms/step - loss: 5.2319e-04 - mae: 0.0169 - val_loss: 0.0046 - val_mae: 0.0573
Epoch 19/30
30/30 [==============================] - 3s 91ms/step - loss: 5.6689e-04 - mae: 0.0172 - val_loss: 0.0045 - val_mae: 0.0563
Epoch 20/30
30/30 [==============================] - 3s 112ms/step - loss: 6.4915e-04 - mae: 0.0186 - val_loss: 0.0073 - val_mae: 0.0752
Epoch 21/30
30/30 [==============================] - 4s 119ms/step - loss: 6.1114e-04 - mae: 0.0180 - val_loss: 0.0039 - val_mae: 0.0520
Epoch 22/30
30/30 [==============================] - 3s 104ms/step - loss: 4.6092e-04 - mae: 0.0158 - val_loss: 0.0029 - val_mae: 0.0432
Epoch 23/30
30/30 [==============================] - 4s 124ms/step - loss: 5.1899e-04 - mae: 0.0165 - val_loss: 0.0046 - val_mae: 0.0577
Epoch 24/30
30/30 [==============================] - 3s 114ms/step - loss: 4.8692e-04 - mae: 0.0158 - val_loss: 0.0049 - val_mae: 0.0594
Epoch 25/30
30/30 [==============================] - 4s 117ms/step - loss: 4.7898e-04 - mae: 0.0157 - val_loss: 0.0032 - val_mae: 0.0469
Epoch 26/30
30/30 [==============================] - 3s 94ms/step - loss: 4.4637e-04 - mae: 0.0154 - val_loss: 0.0017 - val_mae: 0.0313
Epoch 27/30
30/30 [==============================] - 3s 90ms/step - loss: 4.4275e-04 - mae: 0.0155 - val_loss: 0.0037 - val_mae: 0.0521
Epoch 28/30
30/30 [==============================] - 3s 94ms/step - loss: 4.6133e-04 - mae: 0.0154 - val_loss: 0.0013 - val_mae: 0.0280
Epoch 29/30
30/30 [==============================] - 4s 134ms/step - loss: 4.0818e-04 - mae: 0.0147 - val_loss: 0.0038 - val_mae: 0.0524
Epoch 30/30
30/30 [==============================] - 3s 96ms/step - loss: 3.9574e-04 - mae: 0.0144 - val_loss: 0.0014 - val_mae: 0.0286
<keras.src.callbacks.History at 0x78c34a641b10>

LSTM1.history.history.keys()

     
dict_keys(['loss', 'mae', 'val_loss', 'val_mae'])

title = 'Loss and Mean_absolute-error over Epochs'
xlabel='Epochs'
LSTM1_losses = pd.DataFrame(LSTM1.history.history)
ax = LSTM1_losses.plot(figsize=(10,6),title =title)
ax.autoscale(axis = 'x',tight=True)
ax.set(xlabel=xlabel);
     


def highlight_best(data):
  data_highlighted= data.copy()
  min_loss = data_highlighted['loss'].min()
  min_mae = data_highlighted['mae'].min
  min_val_loss = data_highlighted['val_loss'].min()
  min_val_mae = data_highlighted['val_mae'].min()
  min_loass = data_highlighted['loss']==min_loss
  min_mae = data_highlighted['mae']==min_mae
  min_val_loss = data_highlighted['val_loss']==min_val_loss
  min_val_mae = data_highlighted['val_mae']==min_val_mae

  data_highlighted.style.apply(lambda x: ['background: yellow' if v else '' for v in min_loss], subset=['loss']) \
                  .apply(lambda x: ['background: yellow' if v else '' for v in min_mae], subset=['mae']) \
                  .apply(lambda x: ['background: yellow' if v else '' for v in min_val_loss], subset=['val_loss']) \
                  .apply(lambda x: ['background: yellow' if v else '' for v in min_val_mae], subset=['val_mae'])
  return data_highlighted
     

def highlight_best(data):
    data_highlighted = data.copy()

    min_loss = data_highlighted['loss'].min()
    min_mae = data_highlighted['mae'].min()
    min_val_loss = data_highlighted['val_loss'].min()
    min_val_mae = data_highlighted['val_mae'].min()

    data_highlighted['loss_highlight'] = data_highlighted['loss'] == min_loss
    data_highlighted['mae_highlight'] = data_highlighted['mae'] == min_mae
    data_highlighted['val_loss_highlight'] = data_highlighted['val_loss'] == min_val_loss
    data_highlighted['val_mae_highlight'] = data_highlighted['val_mae'] == min_val_mae

    # Apply styling based on boolean masks
    data_highlighted = data_highlighted.style.applymap(lambda v: 'background: yellow' if v else '', subset=['loss_highlight', 'mae_highlight', 'val_loss_highlight', 'val_mae_highlight'])

    return data_highlighted

     

highlighted_LSTM1_losses = highlight_best(LSTM1_losses)
highlighted_LSTM1_losses
     
 	loss	mae	val_loss	val_mae	loss_highlight	mae_highlight	val_loss_highlight	val_mae_highlight
0	0.007134	0.057674	0.019043	0.118394	False	False	False	False
1	0.001324	0.027096	0.008414	0.077868	False	False	False	False
2	0.001114	0.024426	0.007633	0.073055	False	False	False	False
3	0.000971	0.023054	0.004111	0.051222	False	False	False	False
4	0.001028	0.023102	0.004584	0.054945	False	False	False	False
5	0.000858	0.021523	0.002999	0.043116	False	False	False	False
6	0.000927	0.022299	0.003739	0.049695	False	False	False	False
7	0.000834	0.021362	0.005896	0.065895	False	False	False	False
8	0.000799	0.020633	0.004681	0.057882	False	False	False	False
9	0.000745	0.020120	0.002648	0.040512	False	False	False	False
10	0.000750	0.020087	0.004003	0.052444	False	False	False	False
11	0.000704	0.019564	0.004154	0.053860	False	False	False	False
12	0.000686	0.019198	0.007040	0.073510	False	False	False	False
13	0.000666	0.019236	0.003575	0.049789	False	False	False	False
14	0.000619	0.018369	0.002280	0.037657	False	False	False	False
15	0.000648	0.018786	0.001983	0.034649	False	False	False	False
16	0.000607	0.018054	0.003098	0.045591	False	False	False	False
17	0.000523	0.016885	0.004586	0.057264	False	False	False	False
18	0.000567	0.017227	0.004512	0.056348	False	False	False	False
19	0.000649	0.018585	0.007295	0.075180	False	False	False	False
20	0.000611	0.018023	0.003858	0.052015	False	False	False	False
21	0.000461	0.015812	0.002858	0.043212	False	False	False	False
22	0.000519	0.016521	0.004647	0.057745	False	False	False	False
23	0.000487	0.015825	0.004853	0.059424	False	False	False	False
24	0.000479	0.015725	0.003197	0.046943	False	False	False	False
25	0.000446	0.015412	0.001651	0.031333	False	False	False	False
26	0.000443	0.015527	0.003709	0.052063	False	False	False	False
27	0.000461	0.015383	0.001329	0.027965	False	False	True	True
28	0.000408	0.014729	0.003764	0.052361	False	False	False	False
29	0.000396	0.014400	0.001379	0.028613	True	True	False	False

def predict_and_inverse_transform(DF,X_test,model,scaler):
  test = DF.iloc[-len(X_test):].copy()
  predictions = model.predict(X_test)
  inverse_predictions = scaler.inverse_transform(predictions)
  inverse_predictions = pd.DataFrame(inverse_predictions,columns=['Predicted Close','Predicted High','Predicted Low','Predicted Open'],index= DF.iloc[-len(X_test):].index)
  test_df =pd.concat([test.copy(),inverse_predictions],axis = 1)
  test_df[['close','high','low','open']] = scaler.inverse_transform(test_df[['close','high','low','open']])
  return test_df
     

test_df = predict_and_inverse_transform(DF,X_test,LSTM1,scaler)

     
6/6 [==============================] - 1s 33ms/step

plt.figure(figsize=(10,6))
test_df['close'].plot(label='Close Price').autoscale(axis='x',tight=True)
test_df['Predicted Close'].plot(label='Predicted Close Price').autoscale(axis='x',tight=True)
plt.legend()
plt.title('Comparison of Actual and Predicted Close Prices ',backgroundcolor='grey',color='white',fontdict=font2, fontweight='bold')
plt.xlabel('Date',fontdict=font3,labelpad=15)
plt.ylabel('Price',fontdict=font3,labelpad=15)
plt.tight_layout()
plt.grid(True)
     


LSTM2 = Sequential()
LSTM2.add(LSTM(150,input_shape=(length,X_train.shape[2]),return_sequences=True))
LSTM2.add(Dropout(0.2))
LSTM2.add(LSTM(100,input_shape=(length,X_train.shape[2]),return_sequences=True))
LSTM2.add(Dropout(0.2))
LSTM2.add(LSTM(100,input_shape=(length,X_train.shape[2]),return_sequences=False))
LSTM2.add(Dropout(0.2))
LSTM2.add(Dense(units=50))
LSTM2.add(Dense(units=5))
LSTM2.add(Dense(X_train.shape[2]))
LSTM2.compile(optimizer='adam',loss='mse',metrics=['mae'])
LSTM2.summary()
     
Model: "sequential_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 lstm_2 (LSTM)               (None, 60, 150)           93000     
                                                                 
 dropout_2 (Dropout)         (None, 60, 150)           0         
                                                                 
 lstm_3 (LSTM)               (None, 60, 100)           100400    
                                                                 
 dropout_3 (Dropout)         (None, 60, 100)           0         
                                                                 
 lstm_4 (LSTM)               (None, 100)               80400     
                                                                 
 dropout_4 (Dropout)         (None, 100)               0         
                                                                 
 dense_1 (Dense)             (None, 50)                5050      
                                                                 
 dense_2 (Dense)             (None, 5)                 255       
                                                                 
 dense_3 (Dense)             (None, 4)                 24        
                                                                 
=================================================================
Total params: 279129 (1.06 MB)
Trainable params: 279129 (1.06 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________

LSTM2.fit(X_train, y_train,epochs=30,validation_data=(X_test, y_test),batch_size = 32,callbacks=[early_stop],verbose=1)
     
Epoch 1/30
30/30 [==============================] - 15s 258ms/step - loss: 0.0064 - mae: 0.0534 - val_loss: 0.0263 - val_mae: 0.1468
Epoch 2/30
30/30 [==============================] - 8s 257ms/step - loss: 0.0017 - mae: 0.0311 - val_loss: 0.0243 - val_mae: 0.1389
Epoch 3/30
30/30 [==============================] - 6s 198ms/step - loss: 0.0013 - mae: 0.0262 - val_loss: 0.0107 - val_mae: 0.0880
Epoch 4/30
30/30 [==============================] - 8s 258ms/step - loss: 9.8059e-04 - mae: 0.0229 - val_loss: 0.0069 - val_mae: 0.0686
Epoch 5/30
30/30 [==============================] - 6s 201ms/step - loss: 8.2644e-04 - mae: 0.0208 - val_loss: 0.0038 - val_mae: 0.0485
Epoch 6/30
30/30 [==============================] - 8s 255ms/step - loss: 8.0918e-04 - mae: 0.0204 - val_loss: 0.0082 - val_mae: 0.0778
Epoch 7/30
30/30 [==============================] - 6s 209ms/step - loss: 7.0132e-04 - mae: 0.0193 - val_loss: 0.0122 - val_mae: 0.0970
Epoch 8/30
30/30 [==============================] - 8s 269ms/step - loss: 6.4865e-04 - mae: 0.0188 - val_loss: 0.0071 - val_mae: 0.0708
Epoch 9/30
30/30 [==============================] - 6s 209ms/step - loss: 6.6620e-04 - mae: 0.0186 - val_loss: 0.0038 - val_mae: 0.0491
Epoch 10/30
30/30 [==============================] - 8s 257ms/step - loss: 7.5113e-04 - mae: 0.0205 - val_loss: 0.0098 - val_mae: 0.0855
Epoch 11/30
30/30 [==============================] - 6s 202ms/step - loss: 5.6270e-04 - mae: 0.0173 - val_loss: 0.0038 - val_mae: 0.0490
Epoch 12/30
30/30 [==============================] - 8s 263ms/step - loss: 5.9047e-04 - mae: 0.0181 - val_loss: 0.0046 - val_mae: 0.0540
Epoch 13/30
30/30 [==============================] - 6s 201ms/step - loss: 5.7350e-04 - mae: 0.0175 - val_loss: 0.0083 - val_mae: 0.0781
Epoch 14/30
30/30 [==============================] - 8s 268ms/step - loss: 5.2369e-04 - mae: 0.0169 - val_loss: 0.0029 - val_mae: 0.0414
Epoch 15/30
30/30 [==============================] - 6s 213ms/step - loss: 5.4566e-04 - mae: 0.0173 - val_loss: 0.0121 - val_mae: 0.0973
Epoch 16/30
30/30 [==============================] - 8s 252ms/step - loss: 5.2401e-04 - mae: 0.0170 - val_loss: 0.0030 - val_mae: 0.0430
Epoch 17/30
30/30 [==============================] - 6s 206ms/step - loss: 4.3097e-04 - mae: 0.0154 - val_loss: 0.0038 - val_mae: 0.0495
Epoch 18/30
30/30 [==============================] - 8s 265ms/step - loss: 5.1798e-04 - mae: 0.0169 - val_loss: 0.0051 - val_mae: 0.0593
Epoch 19/30
30/30 [==============================] - 6s 217ms/step - loss: 4.5734e-04 - mae: 0.0157 - val_loss: 0.0051 - val_mae: 0.0588
Epoch 20/30
30/30 [==============================] - 7s 246ms/step - loss: 4.2358e-04 - mae: 0.0155 - val_loss: 0.0050 - val_mae: 0.0584
Epoch 21/30
30/30 [==============================] - 7s 228ms/step - loss: 4.1791e-04 - mae: 0.0151 - val_loss: 0.0055 - val_mae: 0.0624
Epoch 22/30
30/30 [==============================] - 7s 226ms/step - loss: 4.0606e-04 - mae: 0.0151 - val_loss: 0.0038 - val_mae: 0.0497
Epoch 23/30
30/30 [==============================] - 7s 251ms/step - loss: 3.8463e-04 - mae: 0.0147 - val_loss: 0.0059 - val_mae: 0.0651
Epoch 24/30
30/30 [==============================] - 6s 205ms/step - loss: 3.6887e-04 - mae: 0.0143 - val_loss: 0.0069 - val_mae: 0.0719
Epoch 25/30
30/30 [==============================] - 8s 268ms/step - loss: 3.5038e-04 - mae: 0.0141 - val_loss: 0.0019 - val_mae: 0.0330
Epoch 26/30
30/30 [==============================] - 7s 218ms/step - loss: 3.5763e-04 - mae: 0.0144 - val_loss: 0.0046 - val_mae: 0.0570
Epoch 27/30
30/30 [==============================] - 8s 273ms/step - loss: 3.5644e-04 - mae: 0.0140 - val_loss: 0.0028 - val_mae: 0.0420
Epoch 28/30
30/30 [==============================] - 6s 212ms/step - loss: 3.7168e-04 - mae: 0.0145 - val_loss: 0.0048 - val_mae: 0.0594
Epoch 29/30
30/30 [==============================] - 8s 266ms/step - loss: 4.2545e-04 - mae: 0.0155 - val_loss: 0.0043 - val_mae: 0.0545
Epoch 30/30
30/30 [==============================] - 6s 200ms/step - loss: 3.4684e-04 - mae: 0.0138 - val_loss: 0.0035 - val_mae: 0.0489
<keras.src.callbacks.History at 0x78c34a500e50>

title=' Loss and Mean Absolute Error vs. Epochs '
xlabel=' Epochs '
LSTM2_losses = pd.DataFrame(LSTM2.history.history)

ax = LSTM2_losses.plot(figsize=(10,6),title=title)
ax.autoscale(axis='x',tight=True)
ax.set(xlabel=xlabel);
     


highlighted_LSTM2_losses = highlight_best(LSTM2_losses)
highlighted_LSTM2_losses
     
 	loss	mae	val_loss	val_mae	loss_highlight	mae_highlight	val_loss_highlight	val_mae_highlight
0	0.006381	0.053377	0.026280	0.146758	False	False	False	False
1	0.001724	0.031147	0.024277	0.138921	False	False	False	False
2	0.001270	0.026150	0.010739	0.087978	False	False	False	False
3	0.000981	0.022858	0.006917	0.068551	False	False	False	False
4	0.000826	0.020842	0.003817	0.048495	False	False	False	False
5	0.000809	0.020360	0.008232	0.077761	False	False	False	False
6	0.000701	0.019271	0.012153	0.097018	False	False	False	False
7	0.000649	0.018811	0.007095	0.070839	False	False	False	False
8	0.000666	0.018565	0.003835	0.049090	False	False	False	False
9	0.000751	0.020528	0.009800	0.085479	False	False	False	False
10	0.000563	0.017271	0.003826	0.049016	False	False	False	False
11	0.000590	0.018092	0.004594	0.054020	False	False	False	False
12	0.000573	0.017546	0.008285	0.078083	False	False	False	False
13	0.000524	0.016893	0.002867	0.041375	False	False	False	False
14	0.000546	0.017349	0.012133	0.097278	False	False	False	False
15	0.000524	0.017024	0.003007	0.043034	False	False	False	False
16	0.000431	0.015437	0.003766	0.049454	False	False	False	False
17	0.000518	0.016865	0.005099	0.059252	False	False	False	False
18	0.000457	0.015737	0.005054	0.058829	False	False	False	False
19	0.000424	0.015468	0.005018	0.058371	False	False	False	False
20	0.000418	0.015070	0.005527	0.062393	False	False	False	False
21	0.000406	0.015058	0.003755	0.049695	False	False	False	False
22	0.000385	0.014653	0.005925	0.065071	False	False	False	False
23	0.000369	0.014275	0.006919	0.071855	False	False	False	False
24	0.000350	0.014123	0.001890	0.033009	False	False	True	True
25	0.000358	0.014353	0.004563	0.057010	False	False	False	False
26	0.000356	0.014022	0.002781	0.042046	False	False	False	False
27	0.000372	0.014548	0.004847	0.059401	False	False	False	False
28	0.000425	0.015514	0.004271	0.054483	False	False	False	False
29	0.000347	0.013824	0.003499	0.048910	True	True	False	False

test_df2 = predict_and_inverse_transform(DF,X_test,LSTM2,scaler)

     
6/6 [==============================] - 2s 95ms/step

plt.figure(figsize=(10, 6))
test_df2['close'].plot(label='Close Price').autoscale(axis='x',tight=True)
test_df2['Predicted Close'].plot(label='Predicted Close Price').autoscale(axis='x',tight=True)

plt.legend()
plt.title('Comparison of Actual and Predicted Close Prices',backgroundcolor='grey',color='white',fontdict=font2, fontweight='bold')
plt.xlabel('Date',fontdict=font3,labelpad=15)
plt.ylabel('Price',fontdict=font3,labelpad=15)
plt.grid(True)
plt.tight_layout()
plt.show()
     


GRU_Model = Sequential()
GRU_Model.add(GRU(128,input_shape=(length,X_train.shape[2]),activation='tanh'))
GRU_Model.add(Dense(X_train.shape[2]))
GRU_Model.compile(optimizer='adam',loss='mse',metrics=['mae'])
GRU_Model.summary()
     
Model: "sequential_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 gru (GRU)                   (None, 128)               51456     
                                                                 
 dense_4 (Dense)             (None, 4)                 516       
                                                                 
=================================================================
Total params: 51972 (203.02 KB)
Trainable params: 51972 (203.02 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________

GRU_Model.fit(X_train, y_train, epochs=30,validation_data=(X_test, y_test),batch_size = 32,callbacks=[early_stop],verbose=1)

     
Epoch 1/30
30/30 [==============================] - 4s 69ms/step - loss: 0.0120 - mae: 0.0759 - val_loss: 0.0388 - val_mae: 0.1748
Epoch 2/30
30/30 [==============================] - 2s 53ms/step - loss: 8.5360e-04 - mae: 0.0228 - val_loss: 0.0071 - val_mae: 0.0700
Epoch 3/30
30/30 [==============================] - 2s 53ms/step - loss: 2.7127e-04 - mae: 0.0125 - val_loss: 0.0010 - val_mae: 0.0242
Epoch 4/30
30/30 [==============================] - 2s 53ms/step - loss: 1.6786e-04 - mae: 0.0092 - val_loss: 7.8539e-04 - val_mae: 0.0208
Epoch 5/30
30/30 [==============================] - 2s 61ms/step - loss: 1.5997e-04 - mae: 0.0090 - val_loss: 5.7609e-04 - val_mae: 0.0177
Epoch 6/30
30/30 [==============================] - 3s 88ms/step - loss: 1.5223e-04 - mae: 0.0087 - val_loss: 5.6962e-04 - val_mae: 0.0177
Epoch 7/30
30/30 [==============================] - 2s 56ms/step - loss: 1.5219e-04 - mae: 0.0087 - val_loss: 6.3436e-04 - val_mae: 0.0188
Epoch 8/30
30/30 [==============================] - 2s 54ms/step - loss: 1.4678e-04 - mae: 0.0086 - val_loss: 5.3455e-04 - val_mae: 0.0171
Epoch 9/30
30/30 [==============================] - 2s 53ms/step - loss: 1.4134e-04 - mae: 0.0083 - val_loss: 5.6755e-04 - val_mae: 0.0176
Epoch 10/30
30/30 [==============================] - 2s 52ms/step - loss: 1.4026e-04 - mae: 0.0083 - val_loss: 5.5385e-04 - val_mae: 0.0174
Epoch 11/30
30/30 [==============================] - 2s 51ms/step - loss: 1.3981e-04 - mae: 0.0083 - val_loss: 5.2042e-04 - val_mae: 0.0171
Epoch 12/30
30/30 [==============================] - 2s 53ms/step - loss: 1.3805e-04 - mae: 0.0083 - val_loss: 6.8083e-04 - val_mae: 0.0195
Epoch 13/30
30/30 [==============================] - 2s 74ms/step - loss: 1.2906e-04 - mae: 0.0080 - val_loss: 4.8980e-04 - val_mae: 0.0165
Epoch 14/30
30/30 [==============================] - 2s 75ms/step - loss: 1.2624e-04 - mae: 0.0079 - val_loss: 5.0032e-04 - val_mae: 0.0167
Epoch 15/30
30/30 [==============================] - 2s 55ms/step - loss: 1.2316e-04 - mae: 0.0078 - val_loss: 5.3683e-04 - val_mae: 0.0173
Epoch 16/30
30/30 [==============================] - 2s 51ms/step - loss: 1.2158e-04 - mae: 0.0077 - val_loss: 6.5524e-04 - val_mae: 0.0197
Epoch 17/30
30/30 [==============================] - 2s 54ms/step - loss: 1.2146e-04 - mae: 0.0077 - val_loss: 5.5036e-04 - val_mae: 0.0179
Epoch 18/30
30/30 [==============================] - 2s 53ms/step - loss: 1.1769e-04 - mae: 0.0076 - val_loss: 5.1777e-04 - val_mae: 0.0171
Epoch 19/30
30/30 [==============================] - 2s 50ms/step - loss: 1.1687e-04 - mae: 0.0076 - val_loss: 4.7678e-04 - val_mae: 0.0163
Epoch 20/30
30/30 [==============================] - 2s 53ms/step - loss: 1.1097e-04 - mae: 0.0073 - val_loss: 5.5419e-04 - val_mae: 0.0178
Epoch 21/30
30/30 [==============================] - 3s 85ms/step - loss: 1.1293e-04 - mae: 0.0075 - val_loss: 6.2260e-04 - val_mae: 0.0191
Epoch 22/30
30/30 [==============================] - 2s 59ms/step - loss: 1.1059e-04 - mae: 0.0073 - val_loss: 5.7161e-04 - val_mae: 0.0183
Epoch 23/30
30/30 [==============================] - 2s 52ms/step - loss: 1.1090e-04 - mae: 0.0073 - val_loss: 7.0205e-04 - val_mae: 0.0206
Epoch 24/30
30/30 [==============================] - 2s 52ms/step - loss: 1.0808e-04 - mae: 0.0072 - val_loss: 5.7968e-04 - val_mae: 0.0184
Epoch 25/30
30/30 [==============================] - 2s 50ms/step - loss: 1.0912e-04 - mae: 0.0074 - val_loss: 5.5672e-04 - val_mae: 0.0178
Epoch 26/30
30/30 [==============================] - 2s 55ms/step - loss: 1.0458e-04 - mae: 0.0071 - val_loss: 6.4520e-04 - val_mae: 0.0196
Epoch 27/30
30/30 [==============================] - 2s 56ms/step - loss: 1.1277e-04 - mae: 0.0075 - val_loss: 5.2671e-04 - val_mae: 0.0172
Epoch 28/30
30/30 [==============================] - 2s 68ms/step - loss: 1.0914e-04 - mae: 0.0073 - val_loss: 4.4402e-04 - val_mae: 0.0160
Epoch 29/30
30/30 [==============================] - 3s 84ms/step - loss: 1.0381e-04 - mae: 0.0071 - val_loss: 4.1836e-04 - val_mae: 0.0153
Epoch 30/30
30/30 [==============================] - 2s 55ms/step - loss: 1.0115e-04 - mae: 0.0070 - val_loss: 5.4710e-04 - val_mae: 0.0179
<keras.src.callbacks.History at 0x78c336f17670>

title=' Loss and Mean Absolute Error vs. Epochs '
xlabel=' Epochs '
GRU_losses = pd.DataFrame(GRU_Model.history.history)

ax = GRU_losses.plot(figsize=(10,6),title=title)
ax.autoscale(axis='x',tight=True)
ax.set(xlabel=xlabel);

     


highlighted_GRU_losses = highlight_best(GRU_losses)
highlighted_GRU_losses
     
 	loss	mae	val_loss	val_mae	loss_highlight	mae_highlight	val_loss_highlight	val_mae_highlight
0	0.012039	0.075876	0.038811	0.174818	False	False	False	False
1	0.000854	0.022841	0.007120	0.070004	False	False	False	False
2	0.000271	0.012539	0.001024	0.024167	False	False	False	False
3	0.000168	0.009176	0.000785	0.020773	False	False	False	False
4	0.000160	0.008955	0.000576	0.017727	False	False	False	False
5	0.000152	0.008692	0.000570	0.017738	False	False	False	False
6	0.000152	0.008735	0.000634	0.018843	False	False	False	False
7	0.000147	0.008563	0.000535	0.017054	False	False	False	False
8	0.000141	0.008348	0.000568	0.017635	False	False	False	False
9	0.000140	0.008324	0.000554	0.017400	False	False	False	False
10	0.000140	0.008348	0.000520	0.017144	False	False	False	False
11	0.000138	0.008277	0.000681	0.019536	False	False	False	False
12	0.000129	0.007994	0.000490	0.016527	False	False	False	False
13	0.000126	0.007917	0.000500	0.016666	False	False	False	False
14	0.000123	0.007779	0.000537	0.017299	False	False	False	False
15	0.000122	0.007726	0.000655	0.019742	False	False	False	False
16	0.000121	0.007733	0.000550	0.017873	False	False	False	False
17	0.000118	0.007623	0.000518	0.017058	False	False	False	False
18	0.000117	0.007560	0.000477	0.016341	False	False	False	False
19	0.000111	0.007336	0.000554	0.017820	False	False	False	False
20	0.000113	0.007482	0.000623	0.019080	False	False	False	False
21	0.000111	0.007317	0.000572	0.018290	False	False	False	False
22	0.000111	0.007296	0.000702	0.020621	False	False	False	False
23	0.000108	0.007237	0.000580	0.018438	False	False	False	False
24	0.000109	0.007370	0.000557	0.017809	False	False	False	False
25	0.000105	0.007103	0.000645	0.019589	False	False	False	False
26	0.000113	0.007486	0.000527	0.017222	False	False	False	False
27	0.000109	0.007336	0.000444	0.015965	False	False	False	False
28	0.000104	0.007137	0.000418	0.015328	False	False	True	True
29	0.000101	0.006981	0.000547	0.017862	True	True	False	False

test_df3 = predict_and_inverse_transform(DF, X_test, GRU_Model, scaler)
     
6/6 [==============================] - 1s 18ms/step

plt.figure(figsize=(10, 6))
test_df3['close'].plot(label='Close Price').autoscale(axis='x',tight=True)
test_df3['Predicted Close'].plot(label='Predicted Close Price').autoscale(axis='x',tight=True)

plt.legend()
plt.title('Comparison of Actual and Predicted Close Prices',backgroundcolor='grey',color='white',fontdict=font2, fontweight='bold')
plt.xlabel('Date',fontdict=font3,labelpad=15)
plt.ylabel('Price',fontdict=font3,labelpad=15)
plt.grid(True)
plt.tight_layout()
     


plt.figure(figsize=(10, 6))
test_df3['high'].plot(label='High Price').autoscale(axis='x',tight=True)
test_df3['Predicted High'].plot(label='Predicted High Price').autoscale(axis='x',tight=True)

plt.legend()
plt.title('Comparison of Actual and Predicted High Prices',backgroundcolor='grey',color='white',fontdict=font2, fontweight='bold')
plt.xlabel('Date',fontdict=font3,labelpad=15)
plt.ylabel('Price',fontdict=font3,labelpad=15)
plt.grid(True)
plt.tight_layout()
     


plt.figure(figsize=(10, 6))
test_df3['low'].plot(label='Low Price').autoscale(axis='x',tight=True)
test_df3['Predicted Low'].plot(label='Predicted Low Price').autoscale(axis='x',tight=True)

plt.legend()
plt.title('Comparison of Actual and Predicted Low Prices',backgroundcolor='grey',color='white',fontdict=font2, fontweight='bold')
plt.xlabel('Date',fontdict=font3,labelpad=15)
plt.ylabel('Price',fontdict=font3,labelpad=15)
plt.grid(True)
plt.tight_layout()
     


plt.figure(figsize=(10, 6))
test_df3['open'].plot(label='Open Price').autoscale(axis='x',tight=True)
test_df3['Predicted Open'].plot(label='Predicted Open Price').autoscale(axis='x',tight=True)

plt.legend()
plt.title('Comparison of Actual and Predicted Open Prices',backgroundcolor='grey',color='white',fontdict=font2, fontweight='bold')
plt.xlabel('Date',fontdict=font3,labelpad=15)
plt.ylabel('Price',fontdict=font3,labelpad=15)
plt.grid(True)
plt.tight_layout()
     

NEXT

NEW MODEL


import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dropout, Dense
from plotly.subplots import make_subplots
import plotly.graph_objects as go
import plotly.io as pi
import plotly.express as px
import matplotlib.pyplot as plt
import datetime
from datetime import timedelta
     

daily_changes = df['close'].diff()
fig = px.histogram(daily_changes, nbins=50, title='Histogram of Daily Price Changes')
fig.update_xaxes(title='Daily Price Change')
fig.update_yaxes(title='Frequency')
fig.update_layout(template='plotly_dark')
fig.show()
     

df
     
close	high	low	open	volume	adjClose	adjHigh	adjLow	adjOpen	adjVolume	divCash	splitFactor
Date												
2016-06-14	718.27	722.470	713.1200	716.48	1306065	718.27	722.470	713.1200	716.48	1306065	0.0	1.0
2016-06-15	718.92	722.980	717.3100	719.00	1214517	718.92	722.980	717.3100	719.00	1214517	0.0	1.0
2016-06-16	710.36	716.650	703.2600	714.91	1982471	710.36	716.650	703.2600	714.91	1982471	0.0	1.0
2016-06-17	691.72	708.820	688.4515	708.65	3402357	691.72	708.820	688.4515	708.65	3402357	0.0	1.0
2016-06-20	693.71	702.480	693.4100	698.77	2082538	693.71	702.480	693.4100	698.77	2082538	0.0	1.0
...	...	...	...	...	...	...	...	...	...	...	...	...
2021-06-07	2466.09	2468.000	2441.0725	2451.32	1192453	2466.09	2468.000	2441.0725	2451.32	1192453	0.0	1.0
2021-06-08	2482.85	2494.495	2468.2400	2479.90	1253253	2482.85	2494.495	2468.2400	2479.90	1253253	0.0	1.0
2021-06-09	2491.40	2505.000	2487.3300	2499.50	1006337	2491.40	2505.000	2487.3300	2499.50	1006337	0.0	1.0
2021-06-10	2521.60	2523.260	2494.0000	2494.01	1561733	2521.60	2523.260	2494.0000	2494.01	1561733	0.0	1.0
2021-06-11	2513.93	2526.990	2498.2900	2524.92	1262309	2513.93	2526.990	2498.2900	2524.92	1262309	0.0	1.0
1258 rows × 12 columns


df['20_day_MA']= df['close'].rolling(window=20).mean()
fig = go.Figure(data=[go.Candlestick(x=df.index,open=df['open'],high=df['high'],low=df['low'],close=df['close'],name="Candlesticks",increasing_line_color='green',decreasing_line_color='red',line=dict(width=1),showlegend=False)])
fig.add_trace(go.Scatter(x=df.index,y=df['20_day_MA'],mode ='lines',name='20 day MA',line=dict(color='rgba(255,255,0,0.3)')))
fig.update_layout(xaxis_title="Date",yaxis_title="Price",title="Google Stock Price Analysis",template='plotly_dark')
fig.show()

     

fig = go.Figure()

# Candlestick chart
fig.add_trace(go.Candlestick(
    x=df.index,open=df['open'],high=df['high'],low=df['low'],close=df['close'],name="Candlesticks",increasing_line_color='green',decreasing_line_color='red',line=dict(width=1),showlegend=False
))

# 20-day Moving Average line
fig.add_trace(go.Scatter(x=df.index,y=df['20_day_MA'],mode='lines',name='20 day MA',line=dict(color='rgba(255,255,0,0.3)')
))

# Update layout
fig.update_layout(xaxis_title="Date",yaxis_title="Price",title="Google Stock Price Analysis",template='plotly_dark'
)

# Show the figure
fig.show()
     

df=df.drop('20_day_MA',axis=1)
     

df['date'] = pd.to_datetime(df.index)
df=df[['date','close','high','low','open','volume']]
     

scaler =MinMaxScaler()
normalized_data = df[['open','high','low','close','volume']].copy()
normalized_data = scaler.fit_transform(normalized_data)

     

train_data,test_data = train_test_split(normalized_data,train_size=0.8,shuffle=False)
train_df = pd.DataFrame(train_data, columns=['open', 'high', 'low', 'volume', 'close'])
test_df = pd.DataFrame(test_data, columns=['open', 'high', 'low', 'volume', 'close'])
     



     


     

def generate_sequences(df, seq_length=50):
    X = df[['open', 'high', 'low', 'volume', 'close']].reset_index(drop=True)
    y = df[['open', 'high', 'low', 'volume', 'close']].reset_index(drop=True)

    sequences = []
    labels = []

    for index in range(len(X) - seq_length + 1):
        sequences.append(X.iloc[index : index + seq_length].values)
        labels.append(y.iloc[index + seq_length - 1].values)

    sequences = np.array(sequences)
    labels = np.array(labels)

    return sequences, labels
     

train_sequences, train_labels = generate_sequences(train_df)
test_sequences, test_labels = generate_sequences(test_df)
     

model = Sequential([
    LSTM(units= 50,return_sequences=True,input_shape=(50,5)),
    Dropout(0.2),
    LSTM(units=50,return_sequences=True),
    Dropout(0.2),
    LSTM(units=50),
    Dropout(0.2),
    Dense(units=5)
    ])
     

model.compile(loss='mean_squared_error',optimizer='adam',metrics=['mae'])
model.summary()

     
Model: "sequential_4"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 lstm_8 (LSTM)               (None, 50, 50)            11200     
                                                                 
 dropout_8 (Dropout)         (None, 50, 50)            0         
                                                                 
 lstm_9 (LSTM)               (None, 50, 50)            20200     
                                                                 
 dropout_9 (Dropout)         (None, 50, 50)            0         
                                                                 
 lstm_10 (LSTM)              (None, 50)                20200     
                                                                 
 dropout_10 (Dropout)        (None, 50)                0         
                                                                 
 dense_6 (Dense)             (None, 5)                 255       
                                                                 
=================================================================
Total params: 51855 (202.56 KB)
Trainable params: 51855 (202.56 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________

epochs = 200
batch_size= 32
history = model.fit(
    train_sequences,
    train_labels,
    epochs = epochs,
    batch_size = batch_size,
    validation_data = (test_sequences,test_labels),
    verbose = 1
)
     
Epoch 1/200
30/30 [==============================] - 12s 161ms/step - loss: 0.0124 - mae: 0.0791 - val_loss: 0.0353 - val_mae: 0.1680
Epoch 2/200
30/30 [==============================] - 4s 130ms/step - loss: 0.0048 - mae: 0.0455 - val_loss: 0.0133 - val_mae: 0.0937
Epoch 3/200
30/30 [==============================] - 4s 143ms/step - loss: 0.0042 - mae: 0.0410 - val_loss: 0.0176 - val_mae: 0.1138
Epoch 4/200
30/30 [==============================] - 2s 80ms/step - loss: 0.0040 - mae: 0.0395 - val_loss: 0.0198 - val_mae: 0.1223
Epoch 5/200
30/30 [==============================] - 2s 76ms/step - loss: 0.0041 - mae: 0.0409 - val_loss: 0.0139 - val_mae: 0.0998
Epoch 6/200
30/30 [==============================] - 2s 74ms/step - loss: 0.0038 - mae: 0.0382 - val_loss: 0.0117 - val_mae: 0.0893
Epoch 7/200
30/30 [==============================] - 3s 94ms/step - loss: 0.0037 - mae: 0.0374 - val_loss: 0.0170 - val_mae: 0.1146
Epoch 8/200
30/30 [==============================] - 3s 105ms/step - loss: 0.0036 - mae: 0.0373 - val_loss: 0.0057 - val_mae: 0.0576
Epoch 9/200
30/30 [==============================] - 2s 73ms/step - loss: 0.0036 - mae: 0.0365 - val_loss: 0.0095 - val_mae: 0.0822
Epoch 10/200
30/30 [==============================] - 2s 75ms/step - loss: 0.0035 - mae: 0.0356 - val_loss: 0.0062 - val_mae: 0.0625
Epoch 11/200
30/30 [==============================] - 2s 74ms/step - loss: 0.0034 - mae: 0.0349 - val_loss: 0.0078 - val_mae: 0.0726
Epoch 12/200
30/30 [==============================] - 2s 75ms/step - loss: 0.0034 - mae: 0.0337 - val_loss: 0.0110 - val_mae: 0.0897
Epoch 13/200
30/30 [==============================] - 3s 118ms/step - loss: 0.0033 - mae: 0.0347 - val_loss: 0.0080 - val_mae: 0.0731
Epoch 14/200
30/30 [==============================] - 2s 78ms/step - loss: 0.0032 - mae: 0.0342 - val_loss: 0.0067 - val_mae: 0.0667
Epoch 15/200
30/30 [==============================] - 2s 75ms/step - loss: 0.0031 - mae: 0.0326 - val_loss: 0.0060 - val_mae: 0.0627
Epoch 16/200
30/30 [==============================] - 2s 74ms/step - loss: 0.0030 - mae: 0.0323 - val_loss: 0.0056 - val_mae: 0.0596
Epoch 17/200
30/30 [==============================] - 2s 75ms/step - loss: 0.0028 - mae: 0.0319 - val_loss: 0.0050 - val_mae: 0.0550
Epoch 18/200
30/30 [==============================] - 3s 97ms/step - loss: 0.0027 - mae: 0.0310 - val_loss: 0.0033 - val_mae: 0.0418
Epoch 19/200
30/30 [==============================] - 3s 97ms/step - loss: 0.0027 - mae: 0.0308 - val_loss: 0.0034 - val_mae: 0.0431
Epoch 20/200
30/30 [==============================] - 2s 74ms/step - loss: 0.0026 - mae: 0.0305 - val_loss: 0.0030 - val_mae: 0.0399
Epoch 21/200
30/30 [==============================] - 2s 74ms/step - loss: 0.0025 - mae: 0.0302 - val_loss: 0.0029 - val_mae: 0.0377
Epoch 22/200
30/30 [==============================] - 2s 74ms/step - loss: 0.0023 - mae: 0.0287 - val_loss: 0.0028 - val_mae: 0.0380
Epoch 23/200
30/30 [==============================] - 2s 73ms/step - loss: 0.0022 - mae: 0.0281 - val_loss: 0.0029 - val_mae: 0.0405
Epoch 24/200
30/30 [==============================] - 4s 124ms/step - loss: 0.0019 - mae: 0.0271 - val_loss: 0.0024 - val_mae: 0.0376
Epoch 25/200
30/30 [==============================] - 2s 73ms/step - loss: 0.0018 - mae: 0.0266 - val_loss: 0.0017 - val_mae: 0.0304
Epoch 26/200
30/30 [==============================] - 2s 73ms/step - loss: 0.0014 - mae: 0.0245 - val_loss: 0.0015 - val_mae: 0.0299
Epoch 27/200
30/30 [==============================] - 2s 76ms/step - loss: 0.0014 - mae: 0.0250 - val_loss: 0.0021 - val_mae: 0.0352
Epoch 28/200
30/30 [==============================] - 2s 75ms/step - loss: 0.0014 - mae: 0.0250 - val_loss: 0.0038 - val_mae: 0.0514
Epoch 29/200
30/30 [==============================] - 3s 101ms/step - loss: 0.0011 - mae: 0.0232 - val_loss: 0.0041 - val_mae: 0.0526
Epoch 30/200
30/30 [==============================] - 3s 91ms/step - loss: 9.8522e-04 - mae: 0.0223 - val_loss: 0.0034 - val_mae: 0.0485
Epoch 31/200
30/30 [==============================] - 2s 74ms/step - loss: 0.0010 - mae: 0.0223 - val_loss: 0.0029 - val_mae: 0.0425
Epoch 32/200
30/30 [==============================] - 2s 73ms/step - loss: 9.6618e-04 - mae: 0.0212 - val_loss: 0.0030 - val_mae: 0.0436
Epoch 33/200
30/30 [==============================] - 2s 73ms/step - loss: 9.1536e-04 - mae: 0.0205 - val_loss: 0.0037 - val_mae: 0.0490
Epoch 34/200
30/30 [==============================] - 2s 83ms/step - loss: 9.6460e-04 - mae: 0.0212 - val_loss: 0.0040 - val_mae: 0.0529
Epoch 35/200
30/30 [==============================] - 3s 110ms/step - loss: 9.8424e-04 - mae: 0.0208 - val_loss: 0.0014 - val_mae: 0.0293
Epoch 36/200
30/30 [==============================] - 2s 76ms/step - loss: 9.7009e-04 - mae: 0.0211 - val_loss: 0.0041 - val_mae: 0.0540
Epoch 37/200
30/30 [==============================] - 2s 74ms/step - loss: 8.1208e-04 - mae: 0.0201 - val_loss: 0.0023 - val_mae: 0.0378
Epoch 38/200
30/30 [==============================] - 2s 74ms/step - loss: 8.1833e-04 - mae: 0.0199 - val_loss: 0.0013 - val_mae: 0.0260
Epoch 39/200
30/30 [==============================] - 2s 75ms/step - loss: 8.5876e-04 - mae: 0.0204 - val_loss: 0.0012 - val_mae: 0.0250
Epoch 40/200
30/30 [==============================] - 3s 116ms/step - loss: 8.3023e-04 - mae: 0.0197 - val_loss: 9.2972e-04 - val_mae: 0.0212
Epoch 41/200
30/30 [==============================] - 2s 82ms/step - loss: 7.8742e-04 - mae: 0.0192 - val_loss: 0.0014 - val_mae: 0.0273
Epoch 42/200
30/30 [==============================] - 3s 85ms/step - loss: 6.9362e-04 - mae: 0.0184 - val_loss: 0.0016 - val_mae: 0.0294
Epoch 43/200
30/30 [==============================] - 2s 76ms/step - loss: 7.1759e-04 - mae: 0.0190 - val_loss: 0.0021 - val_mae: 0.0363
Epoch 44/200
30/30 [==============================] - 2s 74ms/step - loss: 7.1248e-04 - mae: 0.0188 - val_loss: 0.0033 - val_mae: 0.0478
Epoch 45/200
30/30 [==============================] - 3s 108ms/step - loss: 7.2040e-04 - mae: 0.0184 - val_loss: 0.0018 - val_mae: 0.0322
Epoch 46/200
30/30 [==============================] - 3s 84ms/step - loss: 6.5074e-04 - mae: 0.0175 - val_loss: 0.0015 - val_mae: 0.0288
Epoch 47/200
30/30 [==============================] - 2s 75ms/step - loss: 6.6538e-04 - mae: 0.0179 - val_loss: 0.0019 - val_mae: 0.0338
Epoch 48/200
30/30 [==============================] - 2s 76ms/step - loss: 7.1658e-04 - mae: 0.0182 - val_loss: 0.0010 - val_mae: 0.0225
Epoch 49/200
30/30 [==============================] - 2s 75ms/step - loss: 6.6356e-04 - mae: 0.0178 - val_loss: 0.0019 - val_mae: 0.0336
Epoch 50/200
30/30 [==============================] - 3s 93ms/step - loss: 5.8813e-04 - mae: 0.0173 - val_loss: 8.3467e-04 - val_mae: 0.0198
Epoch 51/200
30/30 [==============================] - 3s 102ms/step - loss: 5.9101e-04 - mae: 0.0167 - val_loss: 0.0012 - val_mae: 0.0261
Epoch 52/200
30/30 [==============================] - 2s 74ms/step - loss: 5.9088e-04 - mae: 0.0170 - val_loss: 0.0011 - val_mae: 0.0250
Epoch 53/200
30/30 [==============================] - 2s 77ms/step - loss: 6.1463e-04 - mae: 0.0169 - val_loss: 7.8217e-04 - val_mae: 0.0193
Epoch 54/200
30/30 [==============================] - 2s 75ms/step - loss: 6.0747e-04 - mae: 0.0167 - val_loss: 0.0014 - val_mae: 0.0282
Epoch 55/200
30/30 [==============================] - 2s 77ms/step - loss: 5.7200e-04 - mae: 0.0165 - val_loss: 0.0013 - val_mae: 0.0276
Epoch 56/200
30/30 [==============================] - 3s 116ms/step - loss: 5.3193e-04 - mae: 0.0159 - val_loss: 0.0010 - val_mae: 0.0227
Epoch 57/200
30/30 [==============================] - 2s 75ms/step - loss: 5.5310e-04 - mae: 0.0163 - val_loss: 8.7316e-04 - val_mae: 0.0208
Epoch 58/200
30/30 [==============================] - 2s 76ms/step - loss: 5.6374e-04 - mae: 0.0161 - val_loss: 9.4457e-04 - val_mae: 0.0217
Epoch 59/200
30/30 [==============================] - 2s 77ms/step - loss: 5.8638e-04 - mae: 0.0167 - val_loss: 6.5138e-04 - val_mae: 0.0178
Epoch 60/200
30/30 [==============================] - 2s 78ms/step - loss: 4.9217e-04 - mae: 0.0157 - val_loss: 0.0019 - val_mae: 0.0348
Epoch 61/200
30/30 [==============================] - 3s 115ms/step - loss: 5.2992e-04 - mae: 0.0160 - val_loss: 0.0011 - val_mae: 0.0238
Epoch 62/200
30/30 [==============================] - 2s 77ms/step - loss: 4.9921e-04 - mae: 0.0159 - val_loss: 7.3641e-04 - val_mae: 0.0190
Epoch 63/200
30/30 [==============================] - 4s 125ms/step - loss: 5.3921e-04 - mae: 0.0158 - val_loss: 8.1561e-04 - val_mae: 0.0198
Epoch 64/200
30/30 [==============================] - 6s 191ms/step - loss: 5.4447e-04 - mae: 0.0154 - val_loss: 7.7052e-04 - val_mae: 0.0194
Epoch 65/200
30/30 [==============================] - 3s 105ms/step - loss: 4.6852e-04 - mae: 0.0154 - val_loss: 0.0011 - val_mae: 0.0254
Epoch 66/200
30/30 [==============================] - 2s 74ms/step - loss: 5.1513e-04 - mae: 0.0152 - val_loss: 0.0012 - val_mae: 0.0267
Epoch 67/200
30/30 [==============================] - 2s 76ms/step - loss: 4.9465e-04 - mae: 0.0152 - val_loss: 5.9287e-04 - val_mae: 0.0172
Epoch 68/200
30/30 [==============================] - 2s 76ms/step - loss: 4.2343e-04 - mae: 0.0147 - val_loss: 6.3109e-04 - val_mae: 0.0176
Epoch 69/200
30/30 [==============================] - 2s 76ms/step - loss: 4.7258e-04 - mae: 0.0153 - val_loss: 6.7414e-04 - val_mae: 0.0179
Epoch 70/200
30/30 [==============================] - 4s 119ms/step - loss: 4.5659e-04 - mae: 0.0146 - val_loss: 0.0013 - val_mae: 0.0295
Epoch 71/200
30/30 [==============================] - 2s 74ms/step - loss: 4.1401e-04 - mae: 0.0144 - val_loss: 6.3142e-04 - val_mae: 0.0173
Epoch 72/200
30/30 [==============================] - 2s 74ms/step - loss: 4.0970e-04 - mae: 0.0144 - val_loss: 0.0013 - val_mae: 0.0282
Epoch 73/200
30/30 [==============================] - 2s 79ms/step - loss: 4.3761e-04 - mae: 0.0145 - val_loss: 0.0012 - val_mae: 0.0262
Epoch 74/200
30/30 [==============================] - 2s 75ms/step - loss: 4.3120e-04 - mae: 0.0147 - val_loss: 6.4576e-04 - val_mae: 0.0195
Epoch 75/200
30/30 [==============================] - 3s 107ms/step - loss: 3.8570e-04 - mae: 0.0141 - val_loss: 0.0013 - val_mae: 0.0282
Epoch 76/200
30/30 [==============================] - 2s 81ms/step - loss: 3.9909e-04 - mae: 0.0140 - val_loss: 6.0851e-04 - val_mae: 0.0176
Epoch 77/200
30/30 [==============================] - 2s 75ms/step - loss: 3.9392e-04 - mae: 0.0138 - val_loss: 5.8305e-04 - val_mae: 0.0169
Epoch 78/200
30/30 [==============================] - 2s 74ms/step - loss: 4.1498e-04 - mae: 0.0140 - val_loss: 8.7133e-04 - val_mae: 0.0217
Epoch 79/200
30/30 [==============================] - 2s 80ms/step - loss: 3.7060e-04 - mae: 0.0134 - val_loss: 0.0010 - val_mae: 0.0219
Epoch 80/200
30/30 [==============================] - 3s 98ms/step - loss: 3.7717e-04 - mae: 0.0136 - val_loss: 9.1383e-04 - val_mae: 0.0235
Epoch 81/200
30/30 [==============================] - 3s 104ms/step - loss: 3.6489e-04 - mae: 0.0138 - val_loss: 6.2806e-04 - val_mae: 0.0184
Epoch 82/200
30/30 [==============================] - 2s 76ms/step - loss: 4.1770e-04 - mae: 0.0138 - val_loss: 7.6342e-04 - val_mae: 0.0191
Epoch 83/200
30/30 [==============================] - 2s 75ms/step - loss: 4.0856e-04 - mae: 0.0137 - val_loss: 9.1257e-04 - val_mae: 0.0219
Epoch 84/200
30/30 [==============================] - 2s 74ms/step - loss: 3.7573e-04 - mae: 0.0138 - val_loss: 0.0017 - val_mae: 0.0347
Epoch 85/200
30/30 [==============================] - 2s 80ms/step - loss: 3.8457e-04 - mae: 0.0137 - val_loss: 5.4048e-04 - val_mae: 0.0159
Epoch 86/200
30/30 [==============================] - 4s 122ms/step - loss: 4.2196e-04 - mae: 0.0140 - val_loss: 0.0018 - val_mae: 0.0352
Epoch 87/200
30/30 [==============================] - 2s 76ms/step - loss: 3.9247e-04 - mae: 0.0136 - val_loss: 6.8783e-04 - val_mae: 0.0190
Epoch 88/200
30/30 [==============================] - 2s 80ms/step - loss: 3.9397e-04 - mae: 0.0134 - val_loss: 0.0011 - val_mae: 0.0263
Epoch 89/200
30/30 [==============================] - 2s 76ms/step - loss: 3.4881e-04 - mae: 0.0131 - val_loss: 0.0011 - val_mae: 0.0270
Epoch 90/200
30/30 [==============================] - 2s 75ms/step - loss: 3.4683e-04 - mae: 0.0129 - val_loss: 6.9667e-04 - val_mae: 0.0197
Epoch 91/200
30/30 [==============================] - 3s 111ms/step - loss: 3.7123e-04 - mae: 0.0131 - val_loss: 5.4990e-04 - val_mae: 0.0170
Epoch 92/200
30/30 [==============================] - 2s 80ms/step - loss: 3.4682e-04 - mae: 0.0132 - val_loss: 6.1065e-04 - val_mae: 0.0172
Epoch 93/200
30/30 [==============================] - 2s 77ms/step - loss: 3.1993e-04 - mae: 0.0129 - val_loss: 0.0012 - val_mae: 0.0279
Epoch 94/200
30/30 [==============================] - 2s 76ms/step - loss: 3.2735e-04 - mae: 0.0127 - val_loss: 8.3194e-04 - val_mae: 0.0226
Epoch 95/200
30/30 [==============================] - 2s 81ms/step - loss: 3.4396e-04 - mae: 0.0130 - val_loss: 0.0012 - val_mae: 0.0287
Epoch 96/200
30/30 [==============================] - 3s 102ms/step - loss: 3.2647e-04 - mae: 0.0129 - val_loss: 4.3309e-04 - val_mae: 0.0153
Epoch 97/200
30/30 [==============================] - 3s 92ms/step - loss: 3.4850e-04 - mae: 0.0135 - val_loss: 0.0016 - val_mae: 0.0336
Epoch 98/200
30/30 [==============================] - 2s 75ms/step - loss: 3.5810e-04 - mae: 0.0130 - val_loss: 5.6777e-04 - val_mae: 0.0176
Epoch 99/200
30/30 [==============================] - 2s 78ms/step - loss: 3.1470e-04 - mae: 0.0126 - val_loss: 5.9618e-04 - val_mae: 0.0204
Epoch 100/200
30/30 [==============================] - 2s 82ms/step - loss: 3.4160e-04 - mae: 0.0126 - val_loss: 4.2957e-04 - val_mae: 0.0145
Epoch 101/200
30/30 [==============================] - 3s 92ms/step - loss: 3.1385e-04 - mae: 0.0125 - val_loss: 0.0013 - val_mae: 0.0299
Epoch 102/200
30/30 [==============================] - 3s 99ms/step - loss: 3.3066e-04 - mae: 0.0127 - val_loss: 4.9361e-04 - val_mae: 0.0165
Epoch 103/200
30/30 [==============================] - 2s 79ms/step - loss: 3.0593e-04 - mae: 0.0124 - val_loss: 7.9166e-04 - val_mae: 0.0218
Epoch 104/200
30/30 [==============================] - 2s 76ms/step - loss: 3.3327e-04 - mae: 0.0130 - val_loss: 4.4391e-04 - val_mae: 0.0143
Epoch 105/200
30/30 [==============================] - 2s 76ms/step - loss: 3.3473e-04 - mae: 0.0130 - val_loss: 3.2489e-04 - val_mae: 0.0131
Epoch 106/200
30/30 [==============================] - 2s 83ms/step - loss: 3.4718e-04 - mae: 0.0127 - val_loss: 4.5271e-04 - val_mae: 0.0151
Epoch 107/200
30/30 [==============================] - 3s 111ms/step - loss: 3.0914e-04 - mae: 0.0127 - val_loss: 4.7716e-04 - val_mae: 0.0167
Epoch 108/200
30/30 [==============================] - 2s 78ms/step - loss: 2.9802e-04 - mae: 0.0122 - val_loss: 3.0725e-04 - val_mae: 0.0120
Epoch 109/200
30/30 [==============================] - 2s 77ms/step - loss: 3.3059e-04 - mae: 0.0125 - val_loss: 7.4640e-04 - val_mae: 0.0220
Epoch 110/200
30/30 [==============================] - 2s 77ms/step - loss: 3.3279e-04 - mae: 0.0129 - val_loss: 3.9871e-04 - val_mae: 0.0159
Epoch 111/200
30/30 [==============================] - 2s 81ms/step - loss: 3.3333e-04 - mae: 0.0129 - val_loss: 4.6132e-04 - val_mae: 0.0157
Epoch 112/200
30/30 [==============================] - 4s 120ms/step - loss: 3.1492e-04 - mae: 0.0127 - val_loss: 3.3261e-04 - val_mae: 0.0131
Epoch 113/200
30/30 [==============================] - 2s 77ms/step - loss: 3.1266e-04 - mae: 0.0125 - val_loss: 8.8481e-04 - val_mae: 0.0247
Epoch 114/200
30/30 [==============================] - 2s 82ms/step - loss: 3.1265e-04 - mae: 0.0124 - val_loss: 5.6355e-04 - val_mae: 0.0184
Epoch 115/200
30/30 [==============================] - 2s 78ms/step - loss: 3.5054e-04 - mae: 0.0133 - val_loss: 4.2211e-04 - val_mae: 0.0143
Epoch 116/200
30/30 [==============================] - 2s 81ms/step - loss: 3.5448e-04 - mae: 0.0132 - val_loss: 4.3581e-04 - val_mae: 0.0153
Epoch 117/200
30/30 [==============================] - 4s 123ms/step - loss: 2.9198e-04 - mae: 0.0121 - val_loss: 0.0010 - val_mae: 0.0269
Epoch 118/200
30/30 [==============================] - 3s 83ms/step - loss: 3.4289e-04 - mae: 0.0127 - val_loss: 6.7620e-04 - val_mae: 0.0207
Epoch 119/200
30/30 [==============================] - 2s 77ms/step - loss: 2.9432e-04 - mae: 0.0122 - val_loss: 9.0609e-04 - val_mae: 0.0253
Epoch 120/200
30/30 [==============================] - 2s 79ms/step - loss: 3.2992e-04 - mae: 0.0128 - val_loss: 3.2659e-04 - val_mae: 0.0131
Epoch 121/200
30/30 [==============================] - 2s 78ms/step - loss: 3.3126e-04 - mae: 0.0125 - val_loss: 3.1354e-04 - val_mae: 0.0126
Epoch 122/200
30/30 [==============================] - 3s 111ms/step - loss: 3.0389e-04 - mae: 0.0121 - val_loss: 4.3062e-04 - val_mae: 0.0160
Epoch 123/200
30/30 [==============================] - 3s 87ms/step - loss: 2.8973e-04 - mae: 0.0121 - val_loss: 3.6402e-04 - val_mae: 0.0141
Epoch 124/200
30/30 [==============================] - 2s 75ms/step - loss: 3.0129e-04 - mae: 0.0119 - val_loss: 3.3990e-04 - val_mae: 0.0134
Epoch 125/200
30/30 [==============================] - 2s 75ms/step - loss: 2.6943e-04 - mae: 0.0118 - val_loss: 2.1608e-04 - val_mae: 0.0099
Epoch 126/200
30/30 [==============================] - 2s 75ms/step - loss: 3.0162e-04 - mae: 0.0121 - val_loss: 5.2567e-04 - val_mae: 0.0196
Epoch 127/200
30/30 [==============================] - 3s 97ms/step - loss: 2.9806e-04 - mae: 0.0123 - val_loss: 9.5217e-04 - val_mae: 0.0265
Epoch 128/200
30/30 [==============================] - 3s 104ms/step - loss: 2.9095e-04 - mae: 0.0119 - val_loss: 4.1393e-04 - val_mae: 0.0160
Epoch 129/200
30/30 [==============================] - 2s 74ms/step - loss: 3.0835e-04 - mae: 0.0123 - val_loss: 1.8982e-04 - val_mae: 0.0102
Epoch 130/200
30/30 [==============================] - 2s 74ms/step - loss: 2.8512e-04 - mae: 0.0117 - val_loss: 3.2107e-04 - val_mae: 0.0128
Epoch 131/200
30/30 [==============================] - 2s 75ms/step - loss: 2.7066e-04 - mae: 0.0118 - val_loss: 6.4034e-04 - val_mae: 0.0212
Epoch 132/200
30/30 [==============================] - 2s 78ms/step - loss: 2.8844e-04 - mae: 0.0120 - val_loss: 7.8783e-04 - val_mae: 0.0235
Epoch 133/200
30/30 [==============================] - 4s 125ms/step - loss: 2.6584e-04 - mae: 0.0117 - val_loss: 3.2950e-04 - val_mae: 0.0136
Epoch 134/200
30/30 [==============================] - 4s 129ms/step - loss: 3.2646e-04 - mae: 0.0121 - val_loss: 2.2041e-04 - val_mae: 0.0106
Epoch 135/200
30/30 [==============================] - 2s 75ms/step - loss: 2.9257e-04 - mae: 0.0123 - val_loss: 3.7040e-04 - val_mae: 0.0143
Epoch 136/200
30/30 [==============================] - 5s 157ms/step - loss: 2.7192e-04 - mae: 0.0115 - val_loss: 1.8927e-04 - val_mae: 0.0094
Epoch 137/200
30/30 [==============================] - 4s 130ms/step - loss: 2.7535e-04 - mae: 0.0117 - val_loss: 1.8626e-04 - val_mae: 0.0094
Epoch 138/200
30/30 [==============================] - 2s 78ms/step - loss: 2.7614e-04 - mae: 0.0117 - val_loss: 9.9767e-04 - val_mae: 0.0278
Epoch 139/200
30/30 [==============================] - 4s 127ms/step - loss: 2.7969e-04 - mae: 0.0117 - val_loss: 3.2675e-04 - val_mae: 0.0142
Epoch 140/200
30/30 [==============================] - 2s 81ms/step - loss: 2.6303e-04 - mae: 0.0113 - val_loss: 6.0402e-04 - val_mae: 0.0202
Epoch 141/200
30/30 [==============================] - 3s 114ms/step - loss: 2.6967e-04 - mae: 0.0117 - val_loss: 5.8831e-04 - val_mae: 0.0200
Epoch 142/200
30/30 [==============================] - 3s 88ms/step - loss: 2.5262e-04 - mae: 0.0111 - val_loss: 4.0540e-04 - val_mae: 0.0142
Epoch 143/200
30/30 [==============================] - 2s 77ms/step - loss: 2.7324e-04 - mae: 0.0115 - val_loss: 5.0352e-04 - val_mae: 0.0184
Epoch 144/200
30/30 [==============================] - 2s 75ms/step - loss: 2.8662e-04 - mae: 0.0120 - val_loss: 4.5931e-04 - val_mae: 0.0177
Epoch 145/200
30/30 [==============================] - 2s 81ms/step - loss: 2.8758e-04 - mae: 0.0119 - val_loss: 3.6318e-04 - val_mae: 0.0159
Epoch 146/200
30/30 [==============================] - 3s 100ms/step - loss: 2.7210e-04 - mae: 0.0115 - val_loss: 1.8175e-04 - val_mae: 0.0098
Epoch 147/200
30/30 [==============================] - 3s 91ms/step - loss: 2.8443e-04 - mae: 0.0118 - val_loss: 2.0424e-04 - val_mae: 0.0105
Epoch 148/200
30/30 [==============================] - 2s 77ms/step - loss: 2.6828e-04 - mae: 0.0117 - val_loss: 6.1931e-04 - val_mae: 0.0214
Epoch 149/200
30/30 [==============================] - 2s 77ms/step - loss: 2.8955e-04 - mae: 0.0119 - val_loss: 4.2411e-04 - val_mae: 0.0175
Epoch 150/200
30/30 [==============================] - 2s 78ms/step - loss: 2.8352e-04 - mae: 0.0118 - val_loss: 3.2500e-04 - val_mae: 0.0127
Epoch 151/200
30/30 [==============================] - 3s 96ms/step - loss: 2.7084e-04 - mae: 0.0117 - val_loss: 3.3583e-04 - val_mae: 0.0146
Epoch 152/200
30/30 [==============================] - 3s 99ms/step - loss: 2.4739e-04 - mae: 0.0113 - val_loss: 0.0010 - val_mae: 0.0280
Epoch 153/200
30/30 [==============================] - 2s 79ms/step - loss: 2.7043e-04 - mae: 0.0117 - val_loss: 7.2307e-04 - val_mae: 0.0232
Epoch 154/200
30/30 [==============================] - 2s 78ms/step - loss: 2.7952e-04 - mae: 0.0115 - val_loss: 3.7530e-04 - val_mae: 0.0148
Epoch 155/200
30/30 [==============================] - 4s 129ms/step - loss: 2.7792e-04 - mae: 0.0116 - val_loss: 3.7136e-04 - val_mae: 0.0158
Epoch 156/200
30/30 [==============================] - 4s 120ms/step - loss: 2.6634e-04 - mae: 0.0117 - val_loss: 3.5748e-04 - val_mae: 0.0138
Epoch 157/200
30/30 [==============================] - 2s 75ms/step - loss: 2.6506e-04 - mae: 0.0118 - val_loss: 5.5360e-04 - val_mae: 0.0198
Epoch 158/200
30/30 [==============================] - 2s 76ms/step - loss: 2.5736e-04 - mae: 0.0114 - val_loss: 2.1969e-04 - val_mae: 0.0115
Epoch 159/200
30/30 [==============================] - 2s 77ms/step - loss: 2.5319e-04 - mae: 0.0110 - val_loss: 4.0704e-04 - val_mae: 0.0159
Epoch 160/200
30/30 [==============================] - 2s 79ms/step - loss: 2.9155e-04 - mae: 0.0117 - val_loss: 2.1278e-04 - val_mae: 0.0112
Epoch 161/200
30/30 [==============================] - 3s 109ms/step - loss: 2.5656e-04 - mae: 0.0111 - val_loss: 7.8973e-04 - val_mae: 0.0232
Epoch 162/200
30/30 [==============================] - 3s 86ms/step - loss: 2.7385e-04 - mae: 0.0116 - val_loss: 2.6207e-04 - val_mae: 0.0124
Epoch 163/200
30/30 [==============================] - 2s 77ms/step - loss: 2.6710e-04 - mae: 0.0115 - val_loss: 4.6518e-04 - val_mae: 0.0173
Epoch 164/200
30/30 [==============================] - 2s 78ms/step - loss: 2.5602e-04 - mae: 0.0111 - val_loss: 0.0014 - val_mae: 0.0318
Epoch 165/200
30/30 [==============================] - 2s 77ms/step - loss: 2.4239e-04 - mae: 0.0112 - val_loss: 6.7126e-04 - val_mae: 0.0221
Epoch 166/200
30/30 [==============================] - 3s 99ms/step - loss: 2.5450e-04 - mae: 0.0114 - val_loss: 6.1563e-04 - val_mae: 0.0215
Epoch 167/200
30/30 [==============================] - 3s 93ms/step - loss: 2.7619e-04 - mae: 0.0118 - val_loss: 3.6731e-04 - val_mae: 0.0162
Epoch 168/200
30/30 [==============================] - 2s 76ms/step - loss: 2.3638e-04 - mae: 0.0110 - val_loss: 1.4375e-04 - val_mae: 0.0085
Epoch 169/200
30/30 [==============================] - 2s 75ms/step - loss: 2.9412e-04 - mae: 0.0120 - val_loss: 5.4950e-04 - val_mae: 0.0200
Epoch 170/200
30/30 [==============================] - 2s 77ms/step - loss: 2.6521e-04 - mae: 0.0111 - val_loss: 4.5397e-04 - val_mae: 0.0176
Epoch 171/200
30/30 [==============================] - 3s 87ms/step - loss: 2.6396e-04 - mae: 0.0113 - val_loss: 4.6019e-04 - val_mae: 0.0186
Epoch 172/200
30/30 [==============================] - 3s 112ms/step - loss: 2.7931e-04 - mae: 0.0115 - val_loss: 1.6469e-04 - val_mae: 0.0095
Epoch 173/200
30/30 [==============================] - 2s 76ms/step - loss: 2.3729e-04 - mae: 0.0111 - val_loss: 1.7406e-04 - val_mae: 0.0107
Epoch 174/200
30/30 [==============================] - 2s 77ms/step - loss: 2.3799e-04 - mae: 0.0106 - val_loss: 3.8760e-04 - val_mae: 0.0164
Epoch 175/200
30/30 [==============================] - 2s 78ms/step - loss: 2.2833e-04 - mae: 0.0107 - val_loss: 9.7265e-04 - val_mae: 0.0270
Epoch 176/200
30/30 [==============================] - 2s 78ms/step - loss: 2.5579e-04 - mae: 0.0117 - val_loss: 4.5124e-04 - val_mae: 0.0172
Epoch 177/200
30/30 [==============================] - 4s 125ms/step - loss: 2.7441e-04 - mae: 0.0114 - val_loss: 4.0482e-04 - val_mae: 0.0167
Epoch 178/200
30/30 [==============================] - 2s 77ms/step - loss: 2.6119e-04 - mae: 0.0114 - val_loss: 1.3201e-04 - val_mae: 0.0087
Epoch 179/200
30/30 [==============================] - 2s 76ms/step - loss: 2.4633e-04 - mae: 0.0111 - val_loss: 2.9967e-04 - val_mae: 0.0137
Epoch 180/200
30/30 [==============================] - 2s 76ms/step - loss: 2.4256e-04 - mae: 0.0109 - val_loss: 6.2970e-04 - val_mae: 0.0218
Epoch 181/200
30/30 [==============================] - 2s 75ms/step - loss: 2.4670e-04 - mae: 0.0113 - val_loss: 7.2823e-04 - val_mae: 0.0232
Epoch 182/200
30/30 [==============================] - 3s 102ms/step - loss: 2.4860e-04 - mae: 0.0112 - val_loss: 4.9699e-04 - val_mae: 0.0197
Epoch 183/200
30/30 [==============================] - 3s 91ms/step - loss: 2.3635e-04 - mae: 0.0109 - val_loss: 2.2008e-04 - val_mae: 0.0115
Epoch 184/200
30/30 [==============================] - 2s 77ms/step - loss: 2.6375e-04 - mae: 0.0116 - val_loss: 4.2735e-04 - val_mae: 0.0173
Epoch 185/200
30/30 [==============================] - 2s 82ms/step - loss: 2.7299e-04 - mae: 0.0116 - val_loss: 3.9063e-04 - val_mae: 0.0166
Epoch 186/200
30/30 [==============================] - 2s 78ms/step - loss: 2.4524e-04 - mae: 0.0115 - val_loss: 1.0927e-04 - val_mae: 0.0081
Epoch 187/200
30/30 [==============================] - 3s 99ms/step - loss: 2.3994e-04 - mae: 0.0110 - val_loss: 4.9680e-04 - val_mae: 0.0192
Epoch 188/200
30/30 [==============================] - 3s 107ms/step - loss: 2.6428e-04 - mae: 0.0112 - val_loss: 5.4689e-04 - val_mae: 0.0200
Epoch 189/200
30/30 [==============================] - 2s 78ms/step - loss: 2.8024e-04 - mae: 0.0112 - val_loss: 4.3875e-04 - val_mae: 0.0172
Epoch 190/200
30/30 [==============================] - 2s 77ms/step - loss: 2.5048e-04 - mae: 0.0109 - val_loss: 5.7113e-04 - val_mae: 0.0203
Epoch 191/200
30/30 [==============================] - 2s 77ms/step - loss: 2.6031e-04 - mae: 0.0112 - val_loss: 1.1661e-04 - val_mae: 0.0079
Epoch 192/200
30/30 [==============================] - 2s 80ms/step - loss: 2.5645e-04 - mae: 0.0110 - val_loss: 0.0012 - val_mae: 0.0289
Epoch 193/200
30/30 [==============================] - 3s 117ms/step - loss: 2.8189e-04 - mae: 0.0111 - val_loss: 5.0894e-04 - val_mae: 0.0191
Epoch 194/200
30/30 [==============================] - 2s 78ms/step - loss: 2.5636e-04 - mae: 0.0110 - val_loss: 2.4121e-04 - val_mae: 0.0124
Epoch 195/200
30/30 [==============================] - 2s 83ms/step - loss: 2.4390e-04 - mae: 0.0109 - val_loss: 1.3512e-04 - val_mae: 0.0087
Epoch 196/200
30/30 [==============================] - 2s 76ms/step - loss: 2.6651e-04 - mae: 0.0111 - val_loss: 2.9656e-04 - val_mae: 0.0126
Epoch 197/200
30/30 [==============================] - 2s 75ms/step - loss: 2.4501e-04 - mae: 0.0109 - val_loss: 1.5401e-04 - val_mae: 0.0096
Epoch 198/200
30/30 [==============================] - 4s 121ms/step - loss: 2.7349e-04 - mae: 0.0116 - val_loss: 1.4919e-04 - val_mae: 0.0102
Epoch 199/200
30/30 [==============================] - 2s 78ms/step - loss: 2.2813e-04 - mae: 0.0108 - val_loss: 1.7310e-04 - val_mae: 0.0109
Epoch 200/200
30/30 [==============================] - 2s 80ms/step - loss: 2.5475e-04 - mae: 0.0111 - val_loss: 2.3116e-04 - val_mae: 0.0123

train_predictions = model.predict(train_sequences)
test_predictions = model.predict(test_sequences)

     
30/30 [==============================] - 2s 21ms/step
7/7 [==============================] - 0s 26ms/step

fig = make_subplots(rows =1 ,cols=1,subplot_titles=('Close Prediction'))
train_close_pred = train_predictions[:,0]
train_close_actual= train_labels[:,0]
     

fig.add_trace(go.Scatter(x=np.arange(len(train_close_actual)), y=train_close_actual, mode='lines', name='Actual', opacity=0.9))
fig.add_trace(go.Scatter(x=np.arange(len(train_close_pred)), y=train_close_pred, mode='lines', name='Predicted', opacity=0.6))

fig.update_layout(title='Close Predictions on Train Data', template='plotly_dark')
fig.show()
     

latest_prediction = []
last_seq = test_sequences[:-1]

for _ in range(10):
    prediction = model.predict(last_seq)
    latest_prediction.append(prediction)
     
7/7 [==============================] - 0s 33ms/step
7/7 [==============================] - 0s 25ms/step
7/7 [==============================] - 0s 31ms/step
7/7 [==============================] - 0s 30ms/step
7/7 [==============================] - 0s 27ms/step
7/7 [==============================] - 0s 29ms/step
7/7 [==============================] - 0s 28ms/step
7/7 [==============================] - 0s 30ms/step
7/7 [==============================] - 0s 21ms/step
7/7 [==============================] - 0s 19ms/step

pi.templates.default = "plotly_dark"

predicted_data_next = np.array(latest_prediction).reshape(-1, 5)
last_date = df['date'].max()
next_10_days = [last_date + timedelta(days=i) for i in range(1, 11)]

for i, feature_name in enumerate(['open', 'high', 'low', 'volume', 'close']):
    if feature_name in ['volume', 'close']:
        fig = go.Figure()

        fig.add_trace(go.Scatter(x=next_10_days, y=predicted_data_next[:, i],
                                 mode='lines+markers', name=f'Predicted {feature_name.capitalize()} Prices'))

        fig.update_layout(title=f'Predicted {feature_name.capitalize()} Prices for the Next 10 Days',
                          xaxis_title='Date', yaxis_title=f'{feature_name.capitalize()} Price')

        fig.show()
     
