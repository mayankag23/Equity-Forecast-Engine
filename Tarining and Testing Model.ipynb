
Stock Index Prediction (for next two days given 50 days data for training and testing)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pandas.plotting
import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense, LSTM
from sklearn.preprocessing import MinMaxScaler
%matplotlib inline
data = pd.read_csv(r"C:\Users\HP\Downloads\STOCK_INDEX.csv", index_col = 0, header = 0)
df = pd.DataFrame(data)
df.index = pd.to_datetime(df.index, format = '%Y-%m-%d')
df.isnull().sum()
Open         27
High         27
Low          27
Close        27
Adj Close    27
Volume       27
dtype: int64
Dropping other columns as we are working with only closing price.(It is independent of the others)

df = df[['Close']]
df
Close
Date	
2010-01-04	5232.200195
2010-01-05	5277.899902
2010-01-06	5281.799805
2010-01-07	5263.100098
2010-01-08	5244.750000
...	...
2021-04-26	14485.000000
2021-04-27	14653.049810
2021-04-28	14864.549810
2021-04-29	14894.900390
2021-04-30	14631.099610
2694 rows Ã— 1 columns

rows_with_nan = rows_with_nan = df[df.isnull().any(axis=1)].index
rows_with_nan
DatetimeIndex(['2010-02-06', '2011-05-31', '2011-07-14', '2011-11-24',
               '2011-12-26', '2012-01-02', '2012-01-07', '2012-03-03',
               '2012-05-21', '2012-08-28', '2012-09-08', '2012-11-11',
               '2013-01-01', '2014-01-01', '2014-02-17', '2014-03-22',
               '2014-04-24', '2014-10-15', '2015-01-01', '2015-02-28',
               '2015-04-15', '2016-01-01', '2016-08-12', '2018-01-01',
               '2019-01-01', '2019-10-27', '2020-11-14'],
              dtype='datetime64[ns]', name='Date', freq=None)
df_linear = df.interpolate(method = "linear")
df_cubic = df.interpolate(method = "cubic")
df_splinecubic = df.interpolate(method = "spline", order=3)
#Plot to check which interpolation suits the best by looking at dates which previously had nan values.
%matplotlib notebook
plt.plot(df, label="Actual Data")
plt.plot(df_linear, label = "Linear Interpolation")
plt.plot(df_cubic, label = "Cubic Interpolation", color="red")
plt.plot(df_splinecubic, label="Spline Cubic Interpolation", color="green")
for i in np.arange(0, 27):
    plt.axvline(x=rows_with_nan[i], color='black', linestyle='--')
plt.legend()

<matplotlib.legend.Legend at 0x24350d8bdf0>
Cubic Interpolation is best for our data (although there is very minute difference between cubic and spline cubic interpolations (approx 0.02-0.04)).

df = df.interpolate(method = "cubic")
%matplotlib notebook
df.plot()
plt.ylabel("Price")
plt.legend()

<matplotlib.legend.Legend at 0x1f2865f7070>
ds = df.values
ds
array([[ 5232.200195],
       [ 5277.899902],
       [ 5281.799805],
       ...,
       [14864.54981 ],
       [14894.90039 ],
       [14631.09961 ]])
%matplotlib notebook
plt.plot(ds)

[<matplotlib.lines.Line2D at 0x1f2864a3730>]
#Using MinMaxScaler for normalizing data between 0 & 1
normalizer = MinMaxScaler(feature_range=(0,1))
ds_scaled = normalizer.fit_transform(np.array(ds).reshape(-1,1))
ds_scaled
array([[0.06387819],
       [0.06812123],
       [0.06848332],
       ...,
       [0.95820525],
       [0.96102318],
       [0.93653028]])
len(ds_scaled), len(ds)
(2694, 2694)
#Defining test and train data sizes
train_size = int(len(ds_scaled)*0.80)
test_size = len(ds_scaled) - train_size
train_size,test_size
(2155, 539)
#Splitting data between train and test
ds_train, ds_test = ds_scaled[0:train_size,:], ds_scaled[train_size:len(ds_scaled),:1]
ds_train
array([[0.06387819],
       [0.06812123],
       [0.06848332],
       ...,
       [0.59059468],
       [0.59075716],
       [0.59585436]])
len(ds_train),len(ds_test)
(2155, 539)
#creating dataset in time series for LSTM model
def create_ds(dataset,step):
    Xtrain, Ytrain = [], []
    for i in range(len(dataset)-step-1):
        a = dataset[i:(i+step), 0]
        Xtrain.append(a)
        Ytrain.append(dataset[i + step, 0])
    return np.array(Xtrain), np.array(Ytrain)
#Taking 50 days price as one record for training
time_step = 50
X_train, y_train = create_ds(ds_train,time_step)
X_test, y_test = create_ds(ds_test,time_step)
X_train.shape,y_train.shape
((2104, 50), (2104,))
X_test.shape, y_test.shape
((488, 50), (488,))
#Reshaping data to fit into LSTM model
X_train = X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)
X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)
# np.random.seed(0)
# tf.random.set_seed(0)
#Creating LSTM model using keras
model = keras.models.Sequential()
model.add(LSTM(units=50,return_sequences=True,activation='tanh',input_shape=(X_train.shape[1],1)))
model.add(LSTM(units=50,return_sequences=True))
model.add(LSTM(units=50))
model.add(Dense(units=1, activation='linear'))
model.summary()
Model: "sequential_12"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 lstm_33 (LSTM)              (None, 50, 50)            10400     
                                                                 
 lstm_34 (LSTM)              (None, 50, 50)            20200     
                                                                 
 lstm_35 (LSTM)              (None, 50)                20200     
                                                                 
 dense_11 (Dense)            (None, 1)                 51        
                                                                 
=================================================================
Total params: 50,851
Trainable params: 50,851
Non-trainable params: 0
_________________________________________________________________
#Training model with adam optimizer and mean squared error loss function
model.compile(loss='mean_squared_error',optimizer='adam')
model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=100,batch_size=64)
Epoch 1/100
33/33 [==============================] - 7s 86ms/step - loss: 0.0133 - val_loss: 0.4409
Epoch 2/100
33/33 [==============================] - 2s 60ms/step - loss: 5.7431e-04 - val_loss: 0.4296
Epoch 3/100
33/33 [==============================] - 2s 61ms/step - loss: 4.1032e-04 - val_loss: 0.4396
Epoch 4/100
33/33 [==============================] - 2s 63ms/step - loss: 3.8367e-04 - val_loss: 0.4454
Epoch 5/100
33/33 [==============================] - 2s 67ms/step - loss: 3.6993e-04 - val_loss: 0.4474
Epoch 6/100
33/33 [==============================] - 2s 64ms/step - loss: 3.7506e-04 - val_loss: 0.4514
Epoch 7/100
33/33 [==============================] - 2s 65ms/step - loss: 3.6531e-04 - val_loss: 0.4561
Epoch 8/100
33/33 [==============================] - 2s 67ms/step - loss: 3.4876e-04 - val_loss: 0.4647
Epoch 9/100
33/33 [==============================] - 2s 67ms/step - loss: 3.4350e-04 - val_loss: 0.4661
Epoch 10/100
33/33 [==============================] - 3s 77ms/step - loss: 3.2990e-04 - val_loss: 0.4734
Epoch 11/100
33/33 [==============================] - 3s 87ms/step - loss: 3.2131e-04 - val_loss: 0.4805
Epoch 12/100
33/33 [==============================] - 3s 91ms/step - loss: 3.2619e-04 - val_loss: 0.4859
Epoch 13/100
33/33 [==============================] - 3s 82ms/step - loss: 3.0021e-04 - val_loss: 0.4927
Epoch 14/100
33/33 [==============================] - 3s 81ms/step - loss: 2.8963e-04 - val_loss: 0.4996
Epoch 15/100
33/33 [==============================] - 3s 80ms/step - loss: 2.9865e-04 - val_loss: 0.5071
Epoch 16/100
33/33 [==============================] - 3s 81ms/step - loss: 3.0500e-04 - val_loss: 0.5115
Epoch 17/100
33/33 [==============================] - 3s 83ms/step - loss: 2.6688e-04 - val_loss: 0.5188
Epoch 18/100
33/33 [==============================] - 3s 87ms/step - loss: 2.7381e-04 - val_loss: 0.5280
Epoch 19/100
33/33 [==============================] - 3s 82ms/step - loss: 2.6425e-04 - val_loss: 0.5340
Epoch 20/100
33/33 [==============================] - 3s 82ms/step - loss: 2.4538e-04 - val_loss: 0.5420
Epoch 21/100
33/33 [==============================] - 3s 82ms/step - loss: 2.3570e-04 - val_loss: 0.5514
Epoch 22/100
33/33 [==============================] - 3s 81ms/step - loss: 2.7562e-04 - val_loss: 0.5532
Epoch 23/100
33/33 [==============================] - 3s 83ms/step - loss: 2.2228e-04 - val_loss: 0.5610
Epoch 24/100
33/33 [==============================] - 3s 86ms/step - loss: 2.1705e-04 - val_loss: 0.5697
Epoch 25/100
33/33 [==============================] - 3s 81ms/step - loss: 2.2071e-04 - val_loss: 0.5769
Epoch 26/100
33/33 [==============================] - 3s 81ms/step - loss: 2.2445e-04 - val_loss: 0.5820
Epoch 27/100
33/33 [==============================] - 3s 81ms/step - loss: 2.3564e-04 - val_loss: 0.5849
Epoch 28/100
33/33 [==============================] - 3s 80ms/step - loss: 2.0594e-04 - val_loss: 0.5878
Epoch 29/100
33/33 [==============================] - 3s 84ms/step - loss: 2.0361e-04 - val_loss: 0.5918
Epoch 30/100
33/33 [==============================] - 3s 86ms/step - loss: 2.0112e-04 - val_loss: 0.5987
Epoch 31/100
33/33 [==============================] - 3s 81ms/step - loss: 2.1850e-04 - val_loss: 0.6011
Epoch 32/100
33/33 [==============================] - 3s 81ms/step - loss: 1.9406e-04 - val_loss: 0.6041
Epoch 33/100
33/33 [==============================] - 3s 83ms/step - loss: 1.9394e-04 - val_loss: 0.6079
Epoch 34/100
33/33 [==============================] - 3s 81ms/step - loss: 2.0446e-04 - val_loss: 0.6118
Epoch 35/100
33/33 [==============================] - 3s 87ms/step - loss: 2.1726e-04 - val_loss: 0.6142
Epoch 36/100
33/33 [==============================] - 3s 85ms/step - loss: 1.8162e-04 - val_loss: 0.6189
Epoch 37/100
33/33 [==============================] - 3s 81ms/step - loss: 1.7620e-04 - val_loss: 0.6229
Epoch 38/100
33/33 [==============================] - 3s 81ms/step - loss: 1.7561e-04 - val_loss: 0.6283
Epoch 39/100
33/33 [==============================] - 3s 81ms/step - loss: 1.8204e-04 - val_loss: 0.6324
Epoch 40/100
33/33 [==============================] - 3s 81ms/step - loss: 1.7326e-04 - val_loss: 0.6411
Epoch 41/100
33/33 [==============================] - 3s 87ms/step - loss: 2.1381e-04 - val_loss: 0.6386
Epoch 42/100
33/33 [==============================] - 3s 86ms/step - loss: 1.8150e-04 - val_loss: 0.6405
Epoch 43/100
33/33 [==============================] - 3s 83ms/step - loss: 1.6212e-04 - val_loss: 0.6527
Epoch 44/100
33/33 [==============================] - 3s 83ms/step - loss: 1.6061e-04 - val_loss: 0.6594
Epoch 45/100
33/33 [==============================] - 3s 83ms/step - loss: 1.5702e-04 - val_loss: 0.6645
Epoch 46/100
33/33 [==============================] - 3s 82ms/step - loss: 1.9765e-04 - val_loss: 0.6657
Epoch 47/100
33/33 [==============================] - 3s 89ms/step - loss: 1.5402e-04 - val_loss: 0.6728
Epoch 48/100
33/33 [==============================] - 3s 83ms/step - loss: 1.5366e-04 - val_loss: 0.6813
Epoch 49/100
33/33 [==============================] - 3s 82ms/step - loss: 1.6003e-04 - val_loss: 0.6817
Epoch 50/100
33/33 [==============================] - 3s 81ms/step - loss: 1.7997e-04 - val_loss: 0.6873
Epoch 51/100
33/33 [==============================] - 3s 81ms/step - loss: 1.6294e-04 - val_loss: 0.6929
Epoch 52/100
33/33 [==============================] - 3s 83ms/step - loss: 1.6281e-04 - val_loss: 0.6967
Epoch 53/100
33/33 [==============================] - 3s 90ms/step - loss: 1.3863e-04 - val_loss: 0.7023
Epoch 54/100
33/33 [==============================] - 3s 82ms/step - loss: 1.3439e-04 - val_loss: 0.7034
Epoch 55/100
33/33 [==============================] - 3s 82ms/step - loss: 1.3249e-04 - val_loss: 0.7130
Epoch 56/100
33/33 [==============================] - 3s 83ms/step - loss: 1.3455e-04 - val_loss: 0.7123
Epoch 57/100
33/33 [==============================] - 3s 81ms/step - loss: 1.2811e-04 - val_loss: 0.7138
Epoch 58/100
33/33 [==============================] - 3s 84ms/step - loss: 1.4626e-04 - val_loss: 0.7132
Epoch 59/100
33/33 [==============================] - 3s 88ms/step - loss: 1.3593e-04 - val_loss: 0.7123
Epoch 60/100
33/33 [==============================] - 3s 82ms/step - loss: 1.2250e-04 - val_loss: 0.7125
Epoch 61/100
33/33 [==============================] - 3s 83ms/step - loss: 1.1594e-04 - val_loss: 0.7134
Epoch 62/100
33/33 [==============================] - 3s 83ms/step - loss: 1.1386e-04 - val_loss: 0.7106
Epoch 63/100
33/33 [==============================] - 3s 83ms/step - loss: 1.1783e-04 - val_loss: 0.7036
Epoch 64/100
33/33 [==============================] - 3s 86ms/step - loss: 1.3292e-04 - val_loss: 0.6927
Epoch 65/100
33/33 [==============================] - 3s 88ms/step - loss: 1.2928e-04 - val_loss: 0.6800
Epoch 66/100
33/33 [==============================] - 3s 83ms/step - loss: 1.0440e-04 - val_loss: 0.6705
Epoch 67/100
33/33 [==============================] - 3s 83ms/step - loss: 1.0392e-04 - val_loss: 0.6569
Epoch 68/100
33/33 [==============================] - 3s 83ms/step - loss: 9.7406e-05 - val_loss: 0.6423
Epoch 69/100
33/33 [==============================] - 3s 83ms/step - loss: 9.5125e-05 - val_loss: 0.6267
Epoch 70/100
33/33 [==============================] - 3s 88ms/step - loss: 9.0987e-05 - val_loss: 0.6092
Epoch 71/100
33/33 [==============================] - 3s 86ms/step - loss: 9.7783e-05 - val_loss: 0.5843
Epoch 72/100
33/33 [==============================] - 3s 83ms/step - loss: 9.3101e-05 - val_loss: 0.5576
Epoch 73/100
33/33 [==============================] - 3s 83ms/step - loss: 9.8385e-05 - val_loss: 0.5227
Epoch 74/100
33/33 [==============================] - 3s 83ms/step - loss: 9.8011e-05 - val_loss: 0.4838
Epoch 75/100
33/33 [==============================] - 3s 83ms/step - loss: 8.1900e-05 - val_loss: 0.4486
Epoch 76/100
33/33 [==============================] - 3s 89ms/step - loss: 8.9376e-05 - val_loss: 0.4142
Epoch 77/100
33/33 [==============================] - 3s 88ms/step - loss: 1.2699e-04 - val_loss: 0.3766
Epoch 78/100
33/33 [==============================] - 3s 84ms/step - loss: 7.6588e-05 - val_loss: 0.3383
Epoch 79/100
33/33 [==============================] - 3s 84ms/step - loss: 8.0106e-05 - val_loss: 0.2997
Epoch 80/100
33/33 [==============================] - 3s 84ms/step - loss: 8.1539e-05 - val_loss: 0.2613
Epoch 81/100
33/33 [==============================] - 3s 84ms/step - loss: 7.5228e-05 - val_loss: 0.2273
Epoch 82/100
33/33 [==============================] - 3s 89ms/step - loss: 7.2528e-05 - val_loss: 0.1954
Epoch 83/100
33/33 [==============================] - 3s 84ms/step - loss: 8.4155e-05 - val_loss: 0.1697
Epoch 84/100
33/33 [==============================] - 3s 87ms/step - loss: 8.0411e-05 - val_loss: 0.1469
Epoch 85/100
33/33 [==============================] - 3s 87ms/step - loss: 7.3034e-05 - val_loss: 0.1230
Epoch 86/100
33/33 [==============================] - 3s 87ms/step - loss: 7.4939e-05 - val_loss: 0.1090
Epoch 87/100
33/33 [==============================] - 3s 90ms/step - loss: 6.4935e-05 - val_loss: 0.0938
Epoch 88/100
33/33 [==============================] - 3s 89ms/step - loss: 6.4650e-05 - val_loss: 0.0810
Epoch 89/100
33/33 [==============================] - 3s 86ms/step - loss: 6.5132e-05 - val_loss: 0.0655
Epoch 90/100
33/33 [==============================] - 3s 88ms/step - loss: 6.0662e-05 - val_loss: 0.0563
Epoch 91/100
33/33 [==============================] - 3s 87ms/step - loss: 5.8889e-05 - val_loss: 0.0492
Epoch 92/100
33/33 [==============================] - 3s 84ms/step - loss: 6.9487e-05 - val_loss: 0.0435
Epoch 93/100
33/33 [==============================] - 3s 90ms/step - loss: 8.1174e-05 - val_loss: 0.0418
Epoch 94/100
33/33 [==============================] - 3s 84ms/step - loss: 6.1494e-05 - val_loss: 0.0372
Epoch 95/100
33/33 [==============================] - 3s 83ms/step - loss: 6.9975e-05 - val_loss: 0.0326
Epoch 96/100
33/33 [==============================] - 3s 83ms/step - loss: 7.0316e-05 - val_loss: 0.0320
Epoch 97/100
33/33 [==============================] - 3s 84ms/step - loss: 6.0736e-05 - val_loss: 0.0306
Epoch 98/100
33/33 [==============================] - 3s 87ms/step - loss: 5.9240e-05 - val_loss: 0.0288
Epoch 99/100
33/33 [==============================] - 3s 90ms/step - loss: 5.8298e-05 - val_loss: 0.0284
Epoch 100/100
33/33 [==============================] - 3s 84ms/step - loss: 5.4131e-05 - val_loss: 0.0284
<keras.callbacks.History at 0x22d3117b340>
#PLotting loss, it shows that loss has decreased significantly and model trained well
%matplotlib notebook
loss = model.history.history['loss']
plt.plot(loss)
plt.legend()

No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
<matplotlib.legend.Legend at 0x1f286890b20>
#Predicitng on train and test data
train_predict = model.predict(X_train)
test_predict = model.predict(X_test)
66/66 [==============================] - 3s 18ms/step
16/16 [==============================] - 0s 18ms/step
#Inverse transform to get actual value
train_predict = normalizer.inverse_transform(train_predict)
test_predict = normalizer.inverse_transform(test_predict)
train_predict.shape, test_predict.shape
((2104, 1), (488, 1))
rmse = np.sqrt (np.mean(((test_predict- y_test)**2)))
rmse
11902.85604065926
#Comparing using visuals
%matplotlib notebook
plt.plot(normalizer.inverse_transform(ds_scaled), label='Actual Data')
plt.plot(train_predict, label='Training Data Prediction')
plt.plot(test_predict, label='Testing Data Prediction')
plt.legend()

<matplotlib.legend.Legend at 0x1f285215f40>
type(train_predict)
numpy.ndarray
len(train_predict)
2104
test = np.vstack((train_predict,test_predict))
test
array([[ 5153.6187],
       [ 5195.1084],
       [ 5223.7983],
       ...,
       [14346.696 ],
       [14458.718 ],
       [14633.011 ]], dtype=float32)
len(test)
2592
train_arr=np.arange(0, 2104)
test_arr=np.arange(2104,2592)
#Combining the predited data to create uniform data visualization
%matplotlib notebook
plt.plot(normalizer.inverse_transform(ds_scaled), label="Actual Data")
# plt.plot(train_arr, train_predict)
# plt.plot(test_arr, test_predict)
plt.plot(test, label="Predicted Data")
plt.legend()

<matplotlib.legend.Legend at 0x1f2867e8c40>
#Differentiating between training and testing data
%matplotlib notebook
plt.plot(normalizer.inverse_transform(ds_scaled), label='Actual Data')
plt.plot(train_arr, train_predict, label='Training Data Prediction')
plt.plot(test_arr, test_predict, label='Testing Data Prediction')
# plt.plot(test)
plt.legend()

<matplotlib.legend.Legend at 0x1f2e6e32c70>
#Saving the model
model.save('Model.h5')
